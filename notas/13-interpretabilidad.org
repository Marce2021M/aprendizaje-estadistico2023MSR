#+TITLE: EST-25134: Aprendizaje Estadístico
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Interpretabilidad~
#+STARTUP: showall
:LATEX_PROPERTIES:
#+OPTIONS: toc:nil date:nil author:nil tasks:nil
#+LANGUAGE: sp
#+LATEX_CLASS: handout
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage[sort,numbers]{natbib}
#+LATEX_HEADER: \usepackage[utf8]{inputenc} 
#+LATEX_HEADER: \usepackage[capitalize]{cleveref}
#+LATEX_HEADER: \decimalpoint
#+LATEX_HEADER:\usepackage{framed}
#+LaTeX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \definecolor{backcolour}{rgb}{.95,0.95,0.92}
#+LaTeX_HEADER: \definecolor{codegray}{rgb}{0.5,0.5,0.5}
#+LaTeX_HEADER: \definecolor{codegreen}{rgb}{0,0.6,0} 
#+LaTeX_HEADER: {}
#+LaTeX_HEADER: {\lstset{language={R},basicstyle={\ttfamily\footnotesize},frame=single,breaklines=true,fancyvrb=true,literate={"}{{\texttt{"}}}1{<-}{{$\bm\leftarrow$}}1{<<-}{{$\bm\twoheadleftarrow$}}1{~}{{$\bm\sim$}}1{<=}{{$\bm\le$}}1{>=}{{$\bm\ge$}}1{!=}{{$\bm\neq$}}1{^}{{$^{\bm\wedge}$}}1{|>}{{$\rhd$}}1,otherkeywords={!=, ~, $, \&, \%/\%, \%*\%, \%\%, <-, <<-, ::, /},extendedchars=false,commentstyle={\ttfamily \itshape\color{codegreen}},stringstyle={\color{red}}}
#+LaTeX_HEADER: {}
#+LATEX_HEADER_EXTRA: \definecolor{shadecolor}{gray}{.95}
#+LATEX_HEADER_EXTRA: \newenvironment{NOTES}{\begin{lrbox}{\mybox}\begin{minipage}{0.95\textwidth}\begin{shaded}}{\end{shaded}\end{minipage}\end{lrbox}\fbox{\usebox{\mybox}}}
#+EXPORT_FILE_NAME: ../docs/13-interpretabilidad.pdf
:END:
#+PROPERTY: header-args:R :session intepretability :exports both :results output org :tangle ../rscripts/13-interpretability.R :mkdirp yes :dir ../ :eval never
#+EXCLUDE_TAGS: toc noexport

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2023 | Interpretabilidad y explicabilidad de modelos predictivos.\\
*Objetivo*: En aplicaciones de modelos predictivos usualmente se consideran modelos con alto poder predictivo. A su vez, estos modelos son altamente complejos y es dificil /explicar/ el cómo una predicción es realizada a consecuencia del vector de atributos en consideración. En esta sección estudiaremos algunas de las nociones de interpretabilidad de modelos.\\
*Lectura recomendada*: Los libros de citet:Biecek2021 y citet:Molnar2020 son dos publicaciones recientes que tratan temas de interpretabilidad y explicabilidad de modelos.
#+END_NOTES

#+begin_src R :exports none :results none
  ## Setup ---------------------------------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)
  library(tidymodels)

  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 4)
  ## Problemas con mi consola en Emacs
  options(pillar.subtle = FALSE)
  options(rlang_backtrace_on_error = "none")
  options(crayon.enabled = FALSE)

  ## Para el tema de ggplot
  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
#+end_src


* Table of Contents                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
  - [[#especificación-del-modelo][Especificación del modelo]]
- [[#interpretabilidad][Interpretabilidad]]
- [[#métodos-de-interpretabilidad-local][Métodos de interpretabilidad local]]
  - [[#para-pensar][Para pensar:]]
- [[#descomposición-secuencial-aditiva][Descomposición secuencial aditiva]]
- [[#valores-shap][Valores SHAP]]
  - [[#método][Método]]
    - [[#definición][Definición:]]
    - [[#para-pensar][Para pensar:]]
    - [[#para-pensar][Para pensar:]]
    - [[#definición][Definición:]]
    - [[#propiedades][Propiedades:]]
  - [[#shap-en-ikea][SHAP en IKEA.]]
- [[#expansiones-lineales-locales][Expansiones lineales locales]]
  - [[#construcción-de-lime][Construcción de LIME]]
  - [[#observaciones][Observaciones]]
- [[#exploración-global-de-modelos][Exploración global de modelos]]
  - [[#perfiles-de-dependencia-parcial][Perfiles de dependencia parcial]]
- [[#conclusiones][Conclusiones]]
:END:

* Introducción

 En aplicaciones de modelos predictivos usualmente se consideran modelos con
 alto poder predictivo. A su vez, estos modelos son altamente complejos y es
 dificil /explicar/ el cómo una predicción es realizada a consecuencia del vector
 de atributos en consideración. En esta sección estudiaremos algunas de las
 nociones de interpretabilidad de modelos.

#+REVEAL: split
Para algunos modelos, como regresión lineal o árboles de decisión, es
 relativamente sencillo interpretar las relaciones entre atributos y
 variable respuesta. 

#+REVEAL: split
Para ilustrar retomaremos el ejemplo de productos de ~Ikea~, el cual es original de:  [[https://juliasilge.com/blog/ikea-prices/][Tune random forests for #TidyTuesday IKEA prices]].

#+begin_src R :exports none :results none
  ## Aplicacion: Precios de IKEA ---------------------------------------------
  ikea <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-11-03/ikea.csv")
#+end_src

#+REVEAL: split
Los datos que tenemos disponibles son los siguientes. 
#+begin_src R :exports both :results org 
  ikea_df <- ikea |>
    select(price, name, category, depth, height, width) |>
    mutate(price = log10(price)) |>
    mutate_if(is.character, factor)

  ikea_df |> print(n = 5)
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 3,694 × 6
  price name                  category      depth height width
  <dbl> <fct>                 <fct>         <dbl>  <dbl> <dbl>
1  2.42 FREKVENS              Bar furniture    NA     99    51
2  3.00 NORDVIKEN             Bar furniture    NA    105    80
3  3.32 NORDVIKEN / NORDVIKEN Bar furniture    NA     NA    NA
4  1.84 STIG                  Bar furniture    50    100    60
5  2.35 NORBERG               Bar furniture    60     43    74
# … with 3,689 more rows
# ℹ Use `print(n = ...)` to see more rows
#+end_src

#+REVEAL: split
Los cuales son sometidos a nuestro típico flujo de trabajo de ajuste de modelos
predictivos junto con un proceso de separación de muestras para métricas de
generalización y selección de hiper-parámetros.

\newpage

#+begin_src R :exports none :results none
  ### Preporocesamiento --------------------------------------------------------
#+end_src

#+begin_src R :exports code :results none 
  set.seed(123)
  ikea_split <- initial_split(ikea_df, strata = price)
  ikea_train <- training(ikea_split)
  ikea_test <- testing(ikea_split)

  set.seed(234)
  ikea_folds <- vfold_cv(ikea_train, strata = price)
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none 
  library(textrecipes)
  ranger_recipe <-
    recipe(formula = price ~ ., data = ikea_train) |>
    step_other(name, category, threshold = 0.01) |>
    step_clean_levels(name, category) |>
    step_impute_knn(depth, height, width)
#+end_src

#+begin_src R :exports code :results none 
  linear_recipe <-
    recipe(formula = price ~ ., data = ikea_train) |>
    step_other(name, category, threshold = 0.01) |>
    step_clean_levels(name, category) |>
    step_impute_knn(depth, height, width) |>
    step_dummy(all_nominal_predictors()) |>
    step_normalize(all_predictors())
#+end_src

** Especificación del modelo

#+begin_src R :exports none :results none
  ### Especificación modelo ----------------------------------------------------
#+end_src

#+begin_src R :exports code :results none 
  linear_spec <-
    linear_reg(penalty = 1e-3) |>
    set_mode("regression") |>
    set_engine("glmnet")

  linear_workflow <-
    workflow() |>
    add_recipe(linear_recipe) |>
    add_model(linear_spec)
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none 
  ranger_spec <-
    rand_forest(trees = 1000) |>
    set_mode("regression") |>
    set_engine("ranger", importance = "impurity")

  ranger_workflow <-
    workflow() |>
    add_recipe(ranger_recipe) |>
    add_model(ranger_spec)
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none
  all_cores <- parallel::detectCores(logical = TRUE) - 1
  library(doParallel)
  cl <- makePSOCKcluster(all_cores)
  registerDoParallel(cl)
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none 
  ikea_lm <- linear_workflow |> fit(data = ikea_train)
  ikea_rf <- ranger_workflow |> fit(data = ikea_train)
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/ikea-compare-predictions.jpeg :exports results :results output graphics file
  augment(ikea_lm, ikea_test) |>
    mutate(.linear = .pred,
           .ranger = predict(ikea_rf, ikea_test) |> pull(.pred)) |>
    select(c(price, 8:9)) |>
    pivot_longer(cols = 2:3, names_to = "model", values_to = "predictions") |>
  ggplot(aes(price, predictions)) +
  geom_point(alpha = .4) +
  facet_wrap(~model) +
  geom_abline(intercept = 0, slope = 1, lty = 2) +
  sin_lineas +
  coord_equal()
#+end_src

#+RESULTS:
[[file:../images/ikea-compare-predictions.jpeg]]

#+begin_src R :exports none :results none
  ## (cambio a slides)
#+end_src

* Interpretabilidad

Iremos explorando los conceptos necesarios para interpretabilidad conforme los necesitemos. Primero necesitaremos herramientas de trabajo desde ~R~, y para esta tarea podeos usar ~lime~, ~vip~ y ~DALEXtra~.

#+REVEAL: split
En general podemos usar:
- ~vip~ para usar métodos basados en algún modelo en particular para aprovechar la estructura del modelo predictivo.
- ~DALEX~ para usar métodos que no requieren de una estrcutura en particular (usaremos ~DALEXtra~ para compatibilidad con ~tidymodels~). 

#+begin_src R :exports code :results none
  library(DALEXtra)
#+end_src

#+REVEAL: split
Para poder comenzar lo que tenemos que hacer es crear los objetos de ~DALEX~
(/moDel Agnostic Language for Exploration and eXplanation/).

#+begin_src R :exports code :results none
  explainer_lm <- 
    explain_tidymodels(
      ikea_lm, 
      data = ikea_train |> select(-price), 
      y    = ikea_train |> pull(price),
      label = "linear model",
      verbose = FALSE
    )
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none
  explainer_rf <- 
    explain_tidymodels(
      ikea_rf, 
      data = ikea_train |> select(-price), 
      y    = ikea_train |> pull(price),
      label = "random forest",
      verbose = FALSE
    )
#+end_src

* Métodos de interpretabilidad local

Los siguientes métodos que veremos son ~métodos locales~ es decir, tomamos una
$x^\star \in \mathcal{X} \subset \mathbb{R}^p$ en particular y exploramos la
respuesta a partir de este punto. Por ejemplo, consideremos como $x^\star$ la
observación donde queremos explorar el modelo.

#+begin_src R :exports both :results org 
  set.seed(123)
  mueble <- ikea_test |> sample_n(1)
  mueble
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 1 × 6
  price name     category                         depth height width
  <dbl> <fct>    <fct>                            <dbl>  <dbl> <dbl>
1  2.98 TYSSEDAL Chests of drawers & drawer units    49    102    67
#+end_src

#+REVEAL: split
Sabemos de modelos lineales que los coeficientes están asociados a las
contribuciones de cada predictor a la respuesta. Usualmente, interpretados bajo
un principio /ceteris paribus/ (interpretado en nuestro contexto: dejando
constantes los demás predictores constantes).

#+begin_src R :exports both :results org
  ikea_lm |> extract_fit_parsnip() |>
    tidy() |>
    print(n = 5)
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 35 × 3
  term        estimate penalty
  <chr>          <dbl>   <dbl>
1 (Intercept)  2.67      0.001
2 depth        0.104     0.001
3 height       0.155     0.001
4 width        0.237     0.001
5 name_bekant  0.00497   0.001
# … with 30 more rows
# ℹ Use `print(n = ...)` to see more rows
#+end_src

*** Para pensar:
:PROPERTIES:
:reveal_background: #00468b
:END:
Un profesional de la estadística les recordaría el concepto de /ceteris paribus/ en el contexto de regresión. ¿Es alrededor del vector $x^\star \in \mathcal{X}$ el que usamos para la interpretación o es alrededor del individuo promedio $\bar{x} \in \mathcal{X}$ el que usamos para interpretar el ajuste?

* Descomposición secuencial aditiva

Recordemos que nuestras predicciones (en regresión) se pueden asociar a la función de regresión
\begin{align}
\hat{f}(x) = \mathbb{E}[y | x]\,,
\end{align}
siempre y cuando utilicemos pérdida cuadrática para realizar el ajuste.

#+REVEAL: split
Por ejemplo en regresión lineal podemos calcular el valor esperado de la respuesta para una observación  $x^\star$ por medio de 
\begin{align}
\mathbb{E}[y | x^\star] = \beta_0 + \beta_1 x^\star_{1} + \cdots + \beta_p x^\star_{p} \,,
\end{align}
donde los  coeficientes $\beta$  se ajustan por $\mathsf{MCO}$.

#+REVEAL: split
En general, podemos calcular cómo cambia el valor esperado de $y$ condicionado en que el atributo $j$ tiene un valor de $X_{j}$ por medio de
\begin{align}
\iota(j, x^\star) = \mathbb{E}[Y|x^\star] - \mathbb{E}_{X_{j}}[ \mathbb{E} [y | X_{j}]]\,,
\end{align}
donde $\iota(j, x^\star)$ mide la importancia de la variable $j$ evaluada en el punto $x^\star$.

#+REVEAL: split
Por ejemplo, en regresión lineal tenemos la expresión particular de
\begin{align}
\iota(j, x^\star) = \beta_j \left( x^\star_j - \mathbb{E}[X_j] \right)\,,
\end{align}
que podemos utilizar para expresar
\begin{align}
\hat{f}(x^\star) = (\mathsf{prediccion\,\,media}) + \sum_{j  = 1}^{p} \iota(j , x^\star)\,.
\end{align}

En general, cuando usamos modelos no lineales podemos pensar en
\begin{align}
\iota(j, x^\star) = \mathbb{E}[Y|x^\star_{1:j}] - \mathbb{E} [y | x^\star_{1:j-1}]\,,
\end{align}
para preservar una suma telescópica como la anterior. 

#+BEGIN_NOTES
Nota como el orden de los atributos afecta la descomposición de la predicción en
términos individuales. Como es de esperarse es un mal resumen cuando hay
interacción entre atributos.
#+END_NOTES

#+REVEAL: split
Una vez que hemos decidido sobre cual individuo (observación o instancia) queremos hacer la expansión podemos usar ~DALEX~ para poder crear métricas de sensibilidad. Para esto utilizamos la función ~predict_parts()~.

#+begin_src R :exports both :results org 
  lm_breakdown <- predict_parts(
    explainer = explainer_lm,
    new_observation = mueble
  )
  lm_breakdown
#+end_src

#+RESULTS:
#+begin_src org
                           contribution
linear model: intercept           2.665
linear model: width = 67         -0.162
linear model: category = 7        0.146
linear model: name = 568         -0.049
linear model: depth = 49          0.022
linear model: height = 102       -0.016
linear model: prediction          2.606
#+end_src

#+REVEAL: split
Lo mismo podemos hacer para nuestro modelo de ~random forest~. En este tipo de
tablas interpretamos cómo cada cambio va alejándonos de nuestro /intercepto/ (la
respuesta promedio de nuestro modelo predictivo).

#+begin_src R :exports both :results org 
  rf_breakdown <- predict_parts(
    explainer = explainer_rf,
    new_observation = mueble
  )
  rf_breakdown
#+end_src

#+RESULTS:
#+begin_src org
                            contribution
random forest: intercept           2.665
random forest: depth = 49          0.082
random forest: width = 67         -0.037
random forest: height = 102        0.111
random forest: name = 568          0.008
random forest: category = 7       -0.033
random forest: prediction          2.795
#+end_src

#+REVEAL: split
La interpretación cambia de acuerdo al orden en como se van presentando los
cambios en los atributos y para esto podemos usar el modelo lineal como una heuristica de orden.

#+begin_src R :exports both :results org 
  rfor_breakdown <- predict_parts(
    explainer = explainer_rf,
    new_observation = mueble,
    order = lm_breakdown$variable_name
  )
  rfor_breakdown
#+end_src

#+RESULTS:
#+begin_src org
                            contribution
random forest: intercept           2.665
random forest: width = 67         -0.063
random forest: category = 7       -0.050
random forest: name = 568         -0.028
random forest: depth = 49          0.183
random forest: height = 102        0.088
random forest: prediction          2.795
#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/breakdown-ikea-rf.jpeg :exports results :results output graphics file
  g2 <- rf_breakdown |> plot() + sin_lineas
  g1 <- lm_breakdown |> plot() + sin_lineas
  g3 <- rfor_breakdown |> plot() + sin_lineas
  g1 + g2 + g3
#+end_src

#+RESULTS:
[[file:../images/breakdown-ikea-rf.jpeg]]

#+REVEAL: split
Podemos utilizar también la siguiente opción para explorar posibles contribuciones derivadas de interacciones. 

#+begin_src R :exports both :results org 
  rfin_breakdown <- predict_parts(
    explainer = explainer_rf,
    new_observation = mueble,
    type = "break_down_interactions"
  )
  rfin_breakdown
#+end_src

#+RESULTS:
#+begin_src org
                                     contribution
random forest: intercept                    2.665
random forest: depth = 49                   0.082
random forest: width:category = 67:7       -0.033
random forest: height = 102                 0.090
random forest: name = 568                  -0.009
random forest: prediction                   2.795
#+end_src

* Valores SHAP

La descomposición aditiva que hemos utilizado es sensible al orden en que
hacemos la expansión de las atribuciones. Un problema latente si existen
interacciones entre los predictores. Lo que haremos en esta sección es promediar
bajo distintos órdenes las contribuciones individuales de los atributos.

#+REVEAL: split
Distintos órdenes, pueden presentar distintos resúmenes. Consideremos las
siguientes gráficas de atribución tomadas de citep:Biecek2021. ¿Qué sucede con
las variables de clase y tarifa?

#+DOWNLOADED: screenshot @ 2023-05-03 12:00:43
#+caption: Imagen tomada citep:Biecek2021.
#+attr_html: :width 1200 :align center
[[file:images/20230503-120043_screenshot.png]]

#+REVEAL: split
Tomar promedios puede ayudar eliminar esta dependencia en el orden. Las barras
(colores rojo y verde) muestran las contribuciones promedio, mientras que los
gráficos de caja y brazos muestran la variabilidad a lo largo de los distintos
órdenes.

#+DOWNLOADED: screenshot @ 2023-05-03 12:04:43
#+attr_html: :width 1200 :align center
#+attr_latex: :width .65\linewidth
[[file:images/20230503-120443_screenshot.png]]


** Método

Usaremos los valores de Shapley como mecanismo explicativo de modelos. El nombre
es por /SHapley Additive exPlanations/ (~SHAP~) el cual usa teoría de juegos y el
cual fue desarrollado para juegos cooperativos.

#+REVEAL: split
Lo que queremos es diagnosticar la magnitud de las contribuciones individuales
a una tarea cooperativa.

*** ~Definición~:
Consideremos la predicción para $x^\star \in \mathbb{R}^p$ por parte del modelo
predictivo $\hat{f}$. Consideremos que para un conjunto de índices $J$ dejamos
fijos los valores de acuerdo a las entradas de $x^\star$. Los índices que se
encuentran en $J^c = \{1, \ldots, p\} \setminus J$ son variables
aleatorias. Entonces denotamos por $\Delta^{\ell | J}(x^\star)$ el cambio en el
valor esperado de las predicciones por incluir la $\ell\text{-ésima}$ variable
en el conjunto fijo de valores.

*** Para pensar:
:PROPERTIES:
:reveal_background: #00468b
:END:
Con la definición de arriba, argumenta que tenemos la siguiente igualdad
\begin{align}
\Delta^{\ell | \emptyset }(x^\star) = \mathbb{E}[\hat{f}(X) | X_\ell = x^\star_\ell] - \mathbb{E}[\hat{f}(X)]\,.
\end{align}

*** Para pensar:
:PROPERTIES:
:reveal_background: #00468b
:END:
Con la notación de arriba, ¿cómo escribirías nuestro término $\iota(j, x^\star)$?

*** ~Definición~:
El valor de Shapley está definido como
\begin{align}
\phi(x^\star, j) = \frac{1}{p!} \sum_{J}^{} \Delta ^{j | \mathcal{I}(J, j)} (x^\star)\,,
\end{align}
donde $\mathcal{I}(J, j)$ denota todos los índices en $J$ que se encuentran antes de $j$.

*** Propiedades:
Los valores de Shapley gozan de las siguientes propiedades.
1. /Simetría/: si dos atributos satisfacen que bajo $S \subseteq \{1, \ldots, p\} \setminus \{j,k\}$
   \begin{align}
   \Delta^{j | S}(x^\star) = \Delta^{k | S}(x^\star)\,,
   \end{align}
   entonces
   \begin{align}
   \phi(x^\star, j ) = \phi(x^\star, k)\,.
   \end{align}  
2. /Dummies/: si alguna variable no contribuye en predicciones entonces su valor de Shapley es 0.
3. /Aditividad/: Si $\hat{f} = \hat{f}_1 + \hat{f}_2$ entonces el valor de Shapley es la suma de los valores de Shapley para cada $\hat{f}_i$.
4. /Capacidad local/: La suma de los valores de Shapley es igual a las predicciones del modelo
   \begin{align}
   \hat{f}(x^\star) - \mathbb{E}[\hat{f}(X)] = \sum_{j = 1}^{p} \phi(x^\star, j)\,.
   \end{align}
   
** SHAP en ~IKEA~. 

#+begin_src R :exports code :results none
  set.seed(1801)
  shap_mueble_rf <- 
    predict_parts(
      explainer = explainer_rf, 
      new_observation = mueble, 
      type = "shap",
      B = 20
    )
#+end_src

#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/shap-ikea-rf.jpeg :exports results :results output graphics file
  shap_mueble_rf |>
  plot()
#+end_src

#+RESULTS:
[[file:../images/shap-ikea-rf.jpeg]]

#+REVEAL: split
Podemos comparar para cada modelo para diagnosticar en cada uno cómo cambian las predicciones.

#+begin_src R :exports code :results none
  set.seed(1801)
  shap_mueble_lm <- 
    predict_parts(
      explainer = explainer_lm, 
      new_observation = mueble, 
      type = "shap",
      B = 20
    )
#+end_src

#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/shap-ikea-rf-lm.jpeg :exports results :results output graphics file
  g.lm <- shap_mueble_lm |>
    plot()
  g.rf <- shap_mueble_rf |>
    plot()

  g.lm + g.rf
#+end_src

#+RESULTS:
[[file:../images/shap-ikea-rf-lm.jpeg]]

* Expansiones lineales locales

Podemos explorar la idea de aproximar un modelo predictivo sumamente complejo por uno altamente transparente. Esta es la idea detras de ~LIME~ (/Local Interpretable Model-agnostic Explanations/).

#+REVEAL: split
Por ejemplo, podemos utilizar la función ~predict_surrogate()~ de la siguiente
manera. Para esto consideraremos otro modelo predictivo.

#+begin_src R :exports code :results none 
  xgb_spec <-
    boost_tree(trees = 1000) |>
    set_mode("regression") |>
    set_engine("xgboost")

  xgb_workflow <-
    workflow() |>
    add_recipe(ranger_recipe |> step_dummy(all_nominal_predictors())) |>
    add_model(xgb_spec)
#+end_src

#+begin_src R :exports code :results none 
  ikea_xgb <- xgb_workflow |> fit(data = ikea_train)
#+end_src

#+REVEAL: split
Con la función de ~explain_tidymodels()~ construimos como antes el objeto
necesario para interactuar.
#+begin_src R :exports code :results none
  explainer_xgb <- 
    explain_tidymodels(
      ikea_xgb, 
      data = ikea_train |> select(-price), 
      y    = ikea_train |> pull(price),
      label = "boosted trees",
      verbose = FALSE
    )
#+end_src

#+REVEAL: split
Es importante definir unas opciones para interactuar con los modelos predictivos.

#+begin_src R :exports code :results none 
  library(lime)
  model_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer
  predict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer
#+end_src


#+REVEAL: split
Con esto podemos ya usar la función ~predict_surrogate()~ que usaremos para crear
nuestra aproximación local del modelo predictivo.
#+begin_src R :exports both :results org
  set.seed(108)
  lime_mueble <- predict_surrogate(
    explainer = explainer_xgb,
    new_observation = mueble,
    n_features = 3,
    n_permutations = 500,
    type = "lime"
  )

  lime_mueble |> print(width = 85)
#+end_src
#+REVEAL: split
#+RESULTS:
#+begin_src org
# A tibble: 3 × 11
  model_type case  model_r2 model_intercept model_prediction feature 
  <chr>      <chr>    <dbl>           <dbl>            <dbl> <chr>   
1 regression 1       0.0401            2.40             2.77 depth   
2 regression 1       0.0401            2.40             2.77 category
3 regression 1       0.0401            2.40             2.77 height  
  feature_value feature_weight feature_desc              data         predi…¹
          <dbl>          <dbl> <chr>                     <list>         <dbl>
1            49         0.226  47 < depth <= 60          <named list>    2.70
2             7         0.215  category = Chests of dra… <named list>    2.70
3           102        -0.0694 83.5 < height <= 128.0    <named list>    2.70
# … with abbreviated variable name ¹​prediction
#+end_src

El modelo lineal descrito arriba realiza predicciones por medio de
\begin{align}
\hat{f}(x_{\mathsf{mueble}}) = 2.40 + 0.226 \times  x_1 + 0.215 \times x_2 -0.07 \times x_3\,.
\end{align}

#+REVEAL: split
#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/lime-xboost.jpeg :exports results :results output graphics file
  lime_mueble |> plot()
#+end_src

#+RESULTS:
[[file:../images/lime-xboost.jpeg]]


** Construcción de ~LIME~

La idea es sencilla. Tenemos un modelo predictivo $\hat{f}(x)$ que hemos ajustado con un conjunto de datos. Ahora lo que buscamos es un modelo /transparente/ $g$ tal que
\begin{align}
\hat{g}(x) = \arg \min_{g \in \mathcal{G}^\star} \|g - \hat{f}\|^2 + \Omega(g)\,,
\end{align}
donde $\mathcal{G}^\star = \{g : \mathcal{N}(x^\star) \subset \mathcal{X} \rightarrow \mathcal{Y}| g \text{ es una función sencilla }\}$,
$\|\cdot\|$ es una norma apropiada para modelos predictivos y $\Omega(\cdot)$ asigna una penalización por
complejidad de $g$.

#+REVEAL: split
- Usualmente restringimos $\mathcal{G}^\star$ a ser un espacio de funciones lineales que incluye sólo $d$ entradas (con $d \ll p$).
- Para generar puntos en $\mathcal{N}(x^\star)$ creamos perturbaciones a partir de $x^\star$ y evaluamos el modelo entrenado $\hat{f}$ en esos puntos.
- Ajustamos un modelo de regresión lineal para el conjunto de datos sintéticos.

** Observaciones

- No hacemos supuestos sobre el modelo $\hat{f}$.
- La representación en menores dimensiones nos ayuda a mantener los atributos en un marco manejable.
- La aproximación sólo es local.
- Puede y ha sido utilizado en modelos de texto y visión por computadora.
- Hay que recordar que sólo es una interpretación del modelo ajustado, no de los datos.
- Nosotros utilizamos sólo las funciones de ~lime~ pero también pueden utilizar ~iml~ o ~localModels~, pueden ver mas [[https://ema.drwhy.ai/LIME.html][aquí]].

* Exploración global de modelos

Los diagnósticos globales de los modelos los hemos presentado en secciones anteriores. Por ejemplo, para
modelos basados en árboles, podemos usar los resúmenes de importancia de las variables (usando la librería ~vip~). En ~DALEX~ también podemos usar la función ~model_parts()~ y definir la métrica que nos interesa. 

#+begin_src R :exports code :results none 
  set.seed(1804)
  vip_rf <- model_parts(explainer_rf, loss_function = loss_root_mean_square)
#+end_src


#+REVEAL: split
De esta manera tenemos un gráfico familiar para nosotros que nos muestra la cómo
afecta una variable la salida promedio del modelo $\hat{f}$.

#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/model-parts-ikea-rf.jpeg :exports results :results output graphics file
  plot(vip_rf)
#+end_src

#+RESULTS:
[[file:../images/model-parts-ikea-rf.jpeg]]

** Perfiles de dependencia parcial

Los resumenes anteriores son dependientes de la estructura del modelo
mismo. Pero en muchas situaciones es de interés poder entender cómo afecta un
atributo la salida esperada de las predicciones. Para esto, utilizaremos
perfiles de dependencia parcial (~PDP~ por sus siglas en inglés).

#+REVEAL: split
Este tipo de mecanismos nos ayudan a comparar grupos de
datos bajo un mismo modelo predictivo, o incluso bajo distintos modelos.

- Si dos modelos, con distinto grado de complejidad, tienen el mismo
  comportamiento entonces puede indicar que el riesgo de sobreajuste es
  reducido.
- Podemos utilizar las relaciones encontradas bajo un modelo mas complejo para
  hacer transformaciones que ayuden la capacidad predictiva de un modelo
  interpretable.
- Nos puede ayudar a entender las capacidades predictivas y comportamiento del
  modelo en regiones de pocas observaciones.


#+REVEAL: split
Alternativamente, podemos construir un gráfico que perfile el cambio de la
salida del modelo predictivo utilizando un conjunto de observaciones. Es decir,
construimos un diagnóstico global a partir de diagnósticos locales.

Este es el concepto de utilizar valores esperados individuales condicionados
(~ICE~) o perfiles /ceteris paribus/ (~CP~).


#+REVEAL: split
Para esto construiremos perfiles para 500 observaciones distintas en nuestro
conjunto de entrenamiento y agregaremos los resultados por medio de la función
~model_profile()~. En esta ocasión nos concentraremos en cómo se comporta la
predicción como función parcial del ancho de los empaques (~width~).

#+begin_src R :exports code :results none
  set.seed(1805)
  pdp_width_lm <- model_profile(explainer_lm, N = 500, variables = "width")
  pdp_width_xgb <- model_profile(explainer_xgb, N = 500, variables = "width")
  pdp_width_rf <- model_profile(explainer_rf, N = 500, variables = "width")
#+end_src

#+begin_src R :exports none :results none
  ggplot_pdp <- function(obj, x) {
  
    p <- 
      as_tibble(obj$agr_profiles) %>%
      mutate(`_label_` = stringr::str_remove(`_label_`, "^[^_]*_")) %>%
      ggplot(aes(`_x_`, `_yhat_`)) +
      geom_line(data = as_tibble(obj$cp_profiles),
                aes(x = {{ x }}, group = `_ids_`),
                linewidth = 0.5, alpha = 0.05, color = "gray50")
  
    num_colors <- n_distinct(obj$agr_profiles$`_label_`)
  
    if (num_colors > 1) {
      p <- p + geom_line(aes(color = `_label_`), linewidth = 1.2, alpha = 0.8)
    } else {
      p <- p + geom_line(color = "midnightblue", linewidth = 1.2, alpha = 0.8)
    }
  
    p
  }

#+end_src

#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/pdp-ikea-forest.jpeg :exports results :results output graphics file
  gpd.lm <- pdp_width_lm |> ggplot_pdp(width) +
    labs(x = "Width", title = "linear model",
         y = "Price", 
         color = NULL) + sin_lineas
  gpd.rf <- pdp_width_rf |> ggplot_pdp(width) +
    labs(x = "Width", 
         y = "Price", title = "random forest",
         color = NULL) + sin_lineas
  gpd.xgb <- pdp_width_xgb |> ggplot_pdp(width) +
    labs(x = "Width", 
         y = "Price", title = "xboost",
         color = NULL) + sin_lineas

  gpd.lm + gpd.rf + gpd.xgb
#+end_src

#+RESULTS:
[[file:../images/pdp-ikea-forest.jpeg]]

#+REVEAL: split
Adicionalmente, podemos utilizar la interacción de variables categóricas y
continuas para explorar cómo afecta el cambio de un atributo en la salida del
modelo en conjunto de datos.
#+begin_src R :exports code :results none
  set.seed(1806)
  pdp_wcat <- model_profile(explainer_rf, N = 1000, 
                           variables = "width", 
                           groups = "category")
#+end_src


#+HEADER: :width 1200 :height 800 :R-dev-args bg="transparent"
#+begin_src R :file images/pdp-groups-ikea-forest.jpeg :exports results :results output graphics file
as_tibble(pdp_wcat$agr_profiles) %>%
  mutate(category = stringr::str_remove(`_label_`, "random forest_")) %>%
  ggplot(aes(`_x_`, `_yhat_`, color = category)) +
  geom_line(data = as_tibble(pdp_wcat$cp_profiles),
            aes(x = width, group = `_ids_`),
            linewidth = 0.5, alpha = 0.1, color = "gray50") +
  geom_line(linewidth = 1.2, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  facet_wrap(~category, ncol = 6) +
  scale_color_brewer(palette = "Dark2") +
  labs(x = "width", 
       y = "price", 
       color = NULL) + sin_lineas
#+end_src
#+REVEAL: split
#+RESULTS:
[[file:../images/pdp-groups-ikea-forest.jpeg]]

* Conclusiones

Las herramientas que presentamos en esta sección es un resumen muy breve de mas
temas que se pueden explorar para entender lo que se puede explicar de un
modelo.

#+REVEAL: split
La notación de esta sección ha sido consistente con la idea: una vez que tengo
entrenado un modelo $\hat{f}$ entonces qué puedo decir sobre la relación entre
los atributos y la respuesta. *No* es una relación de causalidad real entre
atributos y respuesta dado que es una herramienta de post-procesamiento.

#+REVEAL: split
Los mecanismo estudiados en esta sección nos pueden ayudar a mejorar nuestro
ciclo de modelado para incorporar mayor conocimiento en éste.


#+DOWNLOADED: screenshot @ 2023-05-03 19:36:44
#+attr_html: :width 1200 :align center
#+caption: Imagen tomada de citep:Biecek2021.
[[file:images/20230503-193920_screenshot.png]]




bibliographystyle:abbrvnat
bibliography:references.bib

