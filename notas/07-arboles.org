#+TITLE: EST-25134: Aprendizaje Estadístico
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Arboles de decisión~
#+STARTUP: showall
:LATEX_PROPERTIES:
#+OPTIONS: toc:nil date:nil author:nil tasks:nil
#+LANGUAGE: sp
#+LATEX_CLASS: handout
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage[sort,numbers]{natbib}
#+LATEX_HEADER: \usepackage[utf8]{inputenc} 
#+LATEX_HEADER: \usepackage[capitalize]{cleveref}
#+LATEX_HEADER: \decimalpoint
#+LATEX_HEADER:\usepackage{framed}
#+LaTeX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \definecolor{backcolour}{rgb}{.95,0.95,0.92}
#+LaTeX_HEADER: \definecolor{codegray}{rgb}{0.5,0.5,0.5}
#+LaTeX_HEADER: \definecolor{codegreen}{rgb}{0,0.6,0} 
#+LaTeX_HEADER: {}
#+LaTeX_HEADER: {\lstset{language={R},basicstyle={\ttfamily\footnotesize},frame=single,breaklines=true,fancyvrb=true,literate={"}{{\texttt{"}}}1{<-}{{$\bm\leftarrow$}}1{<<-}{{$\bm\twoheadleftarrow$}}1{~}{{$\bm\sim$}}1{<=}{{$\bm\le$}}1{>=}{{$\bm\ge$}}1{!=}{{$\bm\neq$}}1{^}{{$^{\bm\wedge}$}}1{|>}{{$\rhd$}}1,otherkeywords={!=, ~, $, \&, \%/\%, \%*\%, \%\%, <-, <<-, ::, /},extendedchars=false,commentstyle={\ttfamily \itshape\color{codegreen}},stringstyle={\color{red}}}
#+LaTeX_HEADER: {}
#+LATEX_HEADER_EXTRA: \definecolor{shadecolor}{gray}{.95}
#+LATEX_HEADER_EXTRA: \newenvironment{NOTES}{\begin{lrbox}{\mybox}\begin{minipage}{0.95\textwidth}\begin{shaded}}{\end{shaded}\end{minipage}\end{lrbox}\fbox{\usebox{\mybox}}}
#+EXPORT_FILE_NAME: ../docs/07-arboles.pdf
:END:
#+PROPERTY: header-args:R :session arboles :exports both :results output org :tangle ../rscripts/07-arboles.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc latex

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2023 | Árboles de decisión.\\
*Objetivo*: Después de haber estudiado los modelos basados en /splines/ es natural considerar modelos que puedan lidiar con varios atributos y que también ajusten modelos por regiones. En esta sección estudiaremos árboles de decisión como un modelo que logra ambos objetivos. \\
*Lectura recomendada*: Capítulo 8 de citep:James2021. 
#+END_NOTES

#+begin_src R :exports none :results none
  ## Setup ---------------------------------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)

  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 4)
  ## Problemas con mi consola en Emacs
  options(pillar.subtle = FALSE, pillar.width = 75)
  options(rlang_backtrace_on_error = "none")
  options(crayon.enabled = FALSE)

  ## Para el tema de ggplot
  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
#+end_src


#+begin_src R :exports none :results none
  ## Paquetes de arboles
  library(tidymodels)
  library(rpart.plot)
  library(vip)
  library(parttree)
#+end_src

* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
  - [[#motivación][Motivación]]
  - [[#ejemplo-baseball][Ejemplo: Baseball]]
  - [[#terminología][Terminología]]
  - [[#interpretación-del-árbol][Interpretación del árbol]]
- [[#construcción-de-un-árbol-de-decisión][Construcción de un árbol de decisión]]
  - [[#mas-detalles][Mas detalles]]
  - [[#predicciones][Predicciones]]
    - [[#para-pensar][Para pensar:]]
  - [[#error-de-generalización][Error de generalización]]
  - [[#proceso-de-poda][Proceso de poda]]
  - [[#selección-del-mejor-sub-árbol][Selección del mejor sub-árbol]]
  - [[#resumen][Resumen]]
  - [[#ejemplo][Ejemplo:]]
- [[#árboles-de-clasificación][Árboles de clasificación]]
  - [[#métricas-de-ajuste-el-índice-de-gini-y-devianza][Métricas de ajuste: el índice de Gini y devianza]]
  - [[#detalles-sobre-selección-de-variables-y-puntos-de-corte][Detalles sobre selección de variables y puntos de corte]]
  - [[#aplicación-episodios-de-scooby-doo][Aplicación: Episodios de Scooby-Doo]]
- [[#hiper-parámetros][Hiper-parámetros]]
- [[#conclusiones][Conclusiones]]
:END:

* Introducción

En esta sección estudiaremos modelos basados en ~árboles de decisión~. Los cuales
estratifican el espacio de los atributos en regiones sencillas para efectuar
predicciones. Reciben su nombre pues la segmentación que se utiliza para
realizar predicciones es una secuencia de ~decisiones binarias~.


#+REVEAL: split
Los árboles de decisión se utilizan para tareas de ~regresión~ y ~clasificación~.
Son modelos sencillos que pueden interpretarse fácilmente, aunque pueden tener
una capacidad predictiva limitada. Después de esta sección estudiaremos cómo se
pueden utilizar como base para modelos mas complejos. 

** Motivación

¿Cómo identificarías un correo electrónico que debería de ser marcado como /spam/?

#+BEGIN_NOTES
Para resolver esta pregunta podrías pensar en identificar palabras clave en el mensaje o el remitente. Intuitivamente, esto tiene sentido. ¿Pero cómo construirías un modelo predictivo para esta tarea? Un modelo de regresión, como veremos mas adelante, tiene distintas limitantes para resolver la tarea. 
#+END_NOTES

#+begin_src R :exports both :results org
  data(spam, package = "kernlab")
  spam <- spam |> as_tibble() |> mutate(type = relevel(type, ref = "spam"))
  spam |> print(n = 3, width = 75)
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 4,601 × 58
   make address   all num3d   our  over remove internet order  mail receive
  <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl> <dbl> <dbl>   <dbl>
1  0       0.64  0.64     0  0.32  0      0        0     0     0       0   
2  0.21    0.28  0.5      0  0.14  0.28   0.21     0.07  0     0.94    0.21
3  0.06    0     0.71     0  1.23  0.19   0.19     0.12  0.64  0.25    0.38
# … with 4,598 more rows, and 47 more variables: will <dbl>, people <dbl>,
#   report <dbl>, addresses <dbl>, free <dbl>, business <dbl>,
#   email <dbl>, you <dbl>, credit <dbl>, your <dbl>, font <dbl>,
#   num000 <dbl>, money <dbl>, hp <dbl>, hpl <dbl>, george <dbl>,
#   num650 <dbl>, lab <dbl>, labs <dbl>, telnet <dbl>, num857 <dbl>,
#   data <dbl>, num415 <dbl>, num85 <dbl>, technology <dbl>,
#   num1999 <dbl>, parts <dbl>, pm <dbl>, direct <dbl>, cs <dbl>, …
# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names
#+end_src

\newpage
#+REVEAL: split
#+begin_src R :exports both :results org 
  spam |>
    group_by(type) |>
    tally()
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 2 × 2
  type        n
  <fct>   <int>
1 spam     1813
2 nonspam  2788
#+end_src

#+REVEAL: split
#+begin_src R :exports none :results none
  set.seed(108727)
  spam_split <- initial_split(spam, strata = type)
  spam_train <- training(spam_split)
  spam_test <- testing(spam_split)
#+end_src

#+begin_src R :exports code :results none
  logistic_spec <- logistic_reg(penalty = .0003, mixture = 1) |>
    set_engine("glmnet")

  logistic_recipe <- recipe(type ~ ., spam_train)

  logistic_wf <- workflow() |>
    add_recipe(logistic_recipe) |>
    add_model(logistic_spec)
#+end_src

#+begin_src R :exports both :results org 
  fit(logistic_wf, spam_train) |>
    augment(new_data = spam_test) |>
    conf_mat(type, .pred_class)
#+end_src

#+RESULTS:
#+begin_src org
          Truth
Prediction spam nonspam
   spam     409      26
   nonspam   45     671
#+end_src

#+REVEAL: split
#+HEADER: :width 500 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/spam-lasso-coefs.jpeg :exports results :results output graphics file
  fit(logistic_wf, spam_train) |>
  pull_workflow_fit() |>
    vi(lambda = 0.0003) |>
    mutate(
      Importance = abs(Importance),
      Variable = fct_reorder(Variable, Importance)
    ) |> head(20) |> 
    ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
    geom_col() +
    scale_x_continuous(expand = c(0, 0)) +
    labs(y = NULL) + sin_lineas
#+end_src
#+attr_latex: :width .65\linewidth
#+RESULTS:
[[file:../images/spam-lasso-coefs.jpeg]]

#+begin_src R :exports none :results none
  tree_spec <- decision_tree(tree_depth = 2) |>
    set_engine("rpart")

  class_tree_spec <- tree_spec |>
    set_mode("classification")

  class_tree_fit <- class_tree_spec |>
    fit(type ~ ., data = spam_train)
#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/spam-arbol-decision.jpeg :exports results :results output graphics file
  class_tree_fit$fit |>
    rpart.plot(tweak = 2, gap = 0, shadow.col = "gray", branch.lty = 2, extra = 108)
#+end_src

#+RESULTS:
[[file:../images/spam-arbol-decision.jpeg]]

#+begin_src R :exports both :results org 
  class_tree_fit |>
    augment(new_data = spam_test) |>
    conf_mat(type, .pred_class)
#+end_src

#+RESULTS:
#+begin_src org
          Truth
Prediction spam nonspam
   spam     311      49
   nonspam  143     648
#+end_src



** Ejemplo: /Baseball/

Consideremos los datos de salarios de jugadores profesionales de /baseball/. Lo
que queremos es predecir el Sueldo en funciones de las características de
carrera de cada jugador.

#+begin_src R :exports results :results org 
  library(ISLR2)
  hitters <- as_tibble(Hitters) |>
    select(Hits, Years, Salary) |>
    filter(complete.cases(Salary))
  hitters |> print(n = 5)
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 263 × 3
   Hits Years Salary
  <int> <int>  <dbl>
1    81    14  475  
2   130     3  480  
3   141    11  500  
4    87     2   91.5
5   169    11  750  
# … with 258 more rows
# ℹ Use `print(n = ...)` to see more rows
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/baseball-dispersion.jpeg :exports results :results output graphics file
  hitters |>
    ggplot(aes(Years, Hits)) +
    geom_point(aes(color = Salary), size = 4) +
    ## scale_color_gradient2(midpoint = 536, low = "blue", mid = "white", high = "red") +
    scale_color_viridis_c(option = "plasma") +
    sin_leyenda + sin_lineas
#+end_src
#+caption: Salario codificado por color: salarios bajos (azul, morado) y salarios altos (naranja, amarillo). 
#+RESULTS:
[[file:../images/baseball-dispersion.jpeg]]

#+REVEAL: split

Un árbol de decisión nos permitirá hacer predicciones con reglas como se muestra a continuación. 

#+begin_src R :exports results :results org 
  tree_spec <- decision_tree(tree_depth = 2) |>
    set_engine("rpart")

  reg_tree_spec <- tree_spec |>
    set_mode("regression")

  reg_tree_fit <- reg_tree_spec |>
    fit(Salary ~ ., data = hitters)

  reg_tree_fit |>
    extract_fit_engine() |>
    rpart.rules(roundint = FALSE)
#+end_src
#+caption: Segmentacion de datos utilizando un árbol de decisión. 
#+RESULTS:
#+begin_src org
 Salary                                
    226 when Years <  4.5              
    465 when Years >= 4.5 & Hits <  118
    949 when Years >= 4.5 & Hits >= 118
#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/baseball-arbol.jpeg :exports results :results output graphics file
  reg_tree_fit |>
    extract_fit_engine() |>
    rpart.plot(tweak = 2, gap = 0, shadow.col = "gray", branch.lty = 2)
#+end_src
#+caption: Representación grafica de un árbol de decisión. 
#+RESULTS:
[[file:../images/baseball-arbol.jpeg]]

#+REVEAL: split
La representación gráfica del árbol anterior tiene dos ~nodos internos~ (dónde se
toman las decisiones binarias) y tres ~nodos terminales~. El número en cada nodo
representa la respuesta y el porcentaje es el número de observaciones del
conjunto de entrenamiento que se han decantado en cada nodo.

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/baseball-tree-dispersion.jpeg :exports results :results output graphics file
  hitters |>
    ggplot(aes(Years, Hits)) +
    geom_point(aes(color = Salary), size = 4) +
    scale_color_viridis_c(option = "plasma") +
    annotate("rect",
             xmin = -Inf, xmax = 4.5, ymin = -Inf, ymax = Inf,
             alpha = 0, color = "darkred", lty = 2) +
    annotate("rect",
             xmin = 4.5, xmax = Inf, ymin = 118, ymax = Inf,
             alpha = 0, color = "darkred", lty = 2) + 
    sin_leyenda + sin_lineas
#+end_src
#+caption: Salario codificado por color: salarios bajos (azul, morado) y salarios altos (naranja, amarillo). Las líneas punteadas representan las decisiones sobre los atributos.  
#+RESULTS:
[[file:../images/baseball-tree-dispersion.jpeg]]

** Terminología

- Las regiones las denotamos por $R_i$, y son conocidas como los ~nodos terminales~. 
- La representación gráfica usualmente está al revés. Las ~hojas~ están en el fondo del gráfico.
- Las decisiones donde se cortan las regiones se denominan ~nodos internos~.
- Los nodos internos están representados por las decisiones: ~years~ $< 4.5$ o ~hits~ $<118$.

** Interpretación del árbol

- La variable ~years~ es el factor mas importante$^*$ en determinar la respuesta del modelo para ~salary~.
- Si nos fijamos en los jugadores con poca experiencia el número de ~hits~ no influye en la determinación de ~salary~.
- Para jugadores con mas de $5$ años de experiencia, el número de ~hits~
  realizados el año anterior determina el nivel del salario. Para aquellos con
  mas de 5 años jugando, a mayor número de ~hits~ se les asocia mayor ~salary~.

* Construcción de un árbol de decisión

1. Dividimos el espacio de predictores en $J$ regiones mutuamente excluyentes
   $R_i$ con $i = 1, \ldots, J$ donde $R_i \cap R_j = \emptyset$ para toda pareja
   $i,j$.
2. Cada observación se asigna una región $R_j$ donde hacemos la misma
   predicción para todas las observaciones en dicha región. Esto lo hacemos promediando la variable respuesta.

** Mas detalles

- En principio las regiones las podemos construir de cualquier manera. Por
  simplicidad utilizamos regiones rectangulares con cortes perpendiculares a los
  ejes.
- El ~objetivo~ es encontrar las regiones $R_1, \ldots, R_J$ que minimizan el error cuadrático
  \begin{align}
  \sum_{j = 1}^{J} \sum_{i \in R_j}^{} ( y_i - \hat y_{R_j})^2\,,
  \end{align}
  donde $\hat y_{R_j}$ es la respuesta en la $j$ ésima región. 


#+REVEAL: split
- Por supuesto, el problema de considerar todas las posibles particiones en $J$ cajas es un ~problema
  combinatorio~.
- Por lo tanto, tomamos una estrategia ~voraz~ (miope, /greedy/) ~secuencial~ (creciente, /top-down/).
- Es /secuencial/ (creciente) pues construye el árbol tomando las decisiones mas
  amplias. Es decir, cortando en las regiones mas grandes para después
  refinarlas.
- Es /miope/ (voraz) pues cada decisión de corte se toma en cada uno de los pasos
  sin considerar los subsecuentes.

#+REVEAL: split
- Consideramos utilizar el ~predictor~ $X_j$ y utilizar el ~punto de corte~ $s$ de tal manera que resulten las regiones
  \begin{align}
  R_1(j,s) = \{X| X_j < s\}\,, \qquad   R_2(j,s) = \{X| X_j \geq s\}\,,
  \end{align}
  que tengan la máxima reducción de ~RSS~ (en el caso de regresión).
- Consideramos dentro de cada región otra decisión de selección de variable y
  decisión de corte para refinar el espacio de los atributos. 
- El procedimiento continua hasta que se satisface un ~criterio de
  terminación~. Por ejemplo, que todas las regiones tengan a lo más 5
  observaciones o se alcance una profundidad máxima del árbol.

** Predicciones

Las predicciones se realizan tomando el promedio de las respuestas en cada una
de las regiones.  Por lo tanto, para predecir la respuesta en un punto tenemos
que evaluar en dónde se encuentra dicho punto y luego tomar el promedio de los
datos de entrenamiento en dicha región.

#+REVEAL: split
En general, las predicciones de un árbol de decisión se pueden escribir como una ecuación lineal de funciones indicadoras. Por ejemplo, consideremos los datos de los sueldos de los jugadores de /baseball/
\begin{align*}
\hat{y}(x)  = &  \beta_1 I\{\mathsf{Years}(x) \leq 4.5\} + \beta_2 I\{ \mathsf{Years}(x) > 4.5 \,\&\, \mathsf{Hits}(x) \leq 118\} \\
&+  \beta_3 I\{ \mathsf{Years}(x) > 4.5 \,\&\, \mathsf{Hits}(x) >118\}\,.
\end{align*}


#+BEGIN_NOTES
Nota como tenemos una expresión del estilo $h_1(\mathsf{Years} ) + h_2(\mathsf{Years}, \mathsf{Hits})$ donde el segundo término modela la interacción de estas dos variables. Con árboles mas complejos, podemos expresar mas términos de interacción (de orden superior) haciendo cada vez más difícil la interpretación de estos términos.
#+END_NOTES


*** Para pensar:
:PROPERTIES:
:reveal_background: #00468b
:END:
En la [[fig:partition]] ¿qué partición del espacio resulta de un árbol de decisión?

#+DOWNLOADED: screenshot @ 2022-04-04 17:45:20
#+name: fig:partition
#+caption: Dos particiones del espacio de atributos. Imagen tomada de citep:James2021. 
#+attr_html: :width 700 :align center
[[file:images/20220404-174520_screenshot.png]]


#+REVEAL: split
En la [[fig:partition-02]], dos representaciones gráficas del mismo árbol de decisión.

#+DOWNLOADED: screenshot @ 2022-04-04 17:48:11
#+name: fig:partition-02
#+caption: Imagen tomada de citep:James2021. 
#+attr_html: :width 700 :align center
[[file:images/20220404-174811_screenshot.png]]

** Error de generalización

- Si ajustamos un árbol de decisión (/descrito anteriormente/) podemos sobre-ajustar fácilmente los datos de entrenamiento (¿por qué?).
- Un árbol mas pequeño puede tener ~menor varianza~ al costo de tener ~mas sesgo~.
- Podríamos considerar cortes que sólo tengan una mejora de $x\%$ puntos en el ~RSS~.
- Pero nos podríamos quedar cortos, un mal corte inmediato podría ayudar a refinar el árbol en el largo plazo.

** Proceso de poda

- Podemos construir un árbol muy grande $T_0$, y ~podarlo~ para encontrar un ~sub-árbol~ con buenas capacidades predictivas.
- El método de poda que se utiliza es por medio de una ~medida de complejidad~ (/cost complexity pruning/, /weakest link pruning/).
- Consideramos una secuencia de árboles$^*$ indexados por un parámetro $\alpha>0$. Para cada valor de $\alpha$ tenemos un sub-árbol $T(\alpha) \subset T_0$ tal que
  \begin{align}
  R_\alpha(T) = \sum_{m = 1}^{|T|} \sum_{i: x_i \in R_m}^{} (y_i - \hat y_{R_m})^2 + \alpha |T|\,,
  \end{align}
  es lo mas pequeño posible.
- En esta notación $|T|$ denota el número de nodos terminales (regiones) del árbol.
- Nota que $T_0 = T(0)$. Es decir, el árbol con ninguna penalización $\left(\alpha = 0\right)$. 


#+REVEAL: split
- citet:Breiman1984 probó que para cualquier $\alpha$ existe un árbol único con el tamaño mas pequeño posible $T(\alpha)$ que minimiza $R_\alpha(T)$.
- *Consecuencia*: no existen dos árboles de la misma profundidad que tengan el mismo valor de $R_\alpha(T)$. 
  
#+BEGIN_NOTES
Utilizar un validación cruzada implicaría evaluar la capacidad predictiva de
cada sub-árbol posible. Lo cual se traduce en un costo computacional alto. 
#+END_NOTES

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/baseball-arbol-alpha-00.jpeg :exports results :results output graphics file
  tree_spec <- decision_tree(cost_complexity = 1e-6) |>
    set_engine("rpart")

  reg_tree_spec <- tree_spec |>
    set_mode("regression")

  reg_tree_fit <- reg_tree_spec |>
    fit(Salary ~ ., data = hitters)

  reg_tree_fit |>
    extract_fit_engine() |>
    rpart.plot(tweak = 1.2, gap = 0, shadow.col = "gray", branch.lty = 2)
#+end_src
#+caption: Representación grafica de un árbol de decisión. Penalización $\alpha = 10^{-6}$. 
#+RESULTS:
[[file:../images/baseball-arbol-alpha.jpeg]]

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/baseball-arbol-alpha-01.jpeg :exports results :results output graphics file
  tree_spec <- decision_tree(cost_complexity = 1e-3) |>
    set_engine("rpart")

  reg_tree_spec <- tree_spec |>
    set_mode("regression")

  reg_tree_fit <- reg_tree_spec |>
    fit(Salary ~ ., data = hitters)

  reg_tree_fit |>
    extract_fit_engine() |>
    rpart.plot(tweak = 1.2, gap = 0, shadow.col = "gray", branch.lty = 2)
#+end_src
#+caption: Representación grafica de un árbol de decisión. Penalización $\alpha = 10^{-3}$. 
#+RESULTS:
[[file:../images/baseball-arbol-alpha.jpeg]]

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/baseball-arbol-alpha-02.jpeg :exports results :results output graphics file
  tree_spec <- decision_tree(cost_complexity = 1e-2) |>
    set_engine("rpart")

  reg_tree_spec <- tree_spec |>
    set_mode("regression")

  reg_tree_fit <- reg_tree_spec |>
    fit(Salary ~ ., data = hitters)

  reg_tree_fit |>
    extract_fit_engine() |>
    rpart.plot(tweak = 1.2, gap = 0, shadow.col = "gray", branch.lty = 2)
#+end_src
#+caption: Representación grafica de un árbol de decisión. Penalización $\alpha = 10^{-2}$. 
#+RESULTS:
[[file:../images/baseball-arbol-alpha.jpeg]]


#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/baseball-arbol-alpha-03.jpeg :exports results :results output graphics file
  tree_spec <- decision_tree(cost_complexity = 1.5e-2) |>
    set_engine("rpart")

  reg_tree_spec <- tree_spec |>
    set_mode("regression")

  reg_tree_fit <- reg_tree_spec |>
    fit(Salary ~ ., data = hitters)

  reg_tree_fit |>
    extract_fit_engine() |>
    rpart.plot(tweak = 1.2, gap = 0, shadow.col = "gray", branch.lty = 2)
#+end_src
#+caption: Representación grafica de un árbol de decisión. Penalización $\alpha = 10^{-1}$. 
#+RESULTS:
[[file:../images/baseball-arbol-alpha.jpeg]]


** Selección del mejor sub-árbol

- El parámetro $\alpha$ controla el compromiso entre complejidad y ajuste al conjunto de entrenamiento.
- Para cada valor de $\alpha$ existe un árbol asociado $T_\alpha$. Bajo una secuencia $\alpha_1 < \alpha_2 < \ldots$ tenemos una sucesión de árboles en donde cada árbol es óptimo. La prueba la encuentran en (citep:Breiman2017,Ripley1996). 
- Usamos un valor ~óptimo~ de $\hat \alpha$ por medio de $\ldots$
- Después, ajustamos el árbol utilizando $\hat \alpha$ y el conjunto de datos completo. 

** Resumen

- Usamos el conjunto de entrenamiento para ajustar un árbol de decisión. Utilizamos un criterio de paro de acuerdo al número de observaciones en los nodos terminales.
- Usamos poda de árboles considerando una penalización por complejidad y obtenemos una secuencia de árboles indexados por $\alpha$. 
- Usamos validación cruzada con $K$ bloques para escoger $\alpha$.
- Reajustamos utilizando todo el conjunto de datos utilizando la $\hat \alpha$ que encontramos en el procedimiento de validación.

** Ejemplo:

Consideremos los datos descritos en este [[https://juliasilge.com/blog/wind-turbine/][caso de estudio]] por Julia Silge (autora del libro tidymodels). El objetivo es poder predecir la capacidad de las turbinas de viento en Canadá por medio de cierta colección de descriptores. Puedes seguir [[https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-10-27/readme.md][la liga]] para una descripción mas detallada de los datos. 

#+DOWNLOADED: screenshot @ 2022-04-04 18:44:03
#+caption: Imagen tomada de la documentación de los datos [[https://juliasilge.com/blog/wind-turbine/][caso de estudio]]. 
#+attr_html: :width 700 :align center
[[file:images/20220404-184403_screenshot.png]]



#+begin_src R :exports none :results none
   ## Turbinas de viento ------------------------------------
   turbines <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-10-27/wind-turbine.csv")

  turbines_df <- turbines |>
   transmute(
     turbine_capacity = turbine_rated_capacity_k_w,
     rotor_diameter_m,
     hub_height_m,
     commissioning_date = parse_number(commissioning_date),
     province_territory = fct_lump_n(province_territory, 10),
     model = fct_lump_n(model, 10)
   ) |>
   filter(!is.na(turbine_capacity)) |>
   mutate_if(is.character, factor)
#+end_src

#+REVEAL: split
¿Cómo se relacionan las características como año de producción o tamaño de la turbina con su capacidad energética?

#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/arboles-turbinas-exploratorio.jpeg :exports results :results output graphics file
  turbines_df |>
    select(turbine_capacity:commissioning_date) |>
    pivot_longer(rotor_diameter_m:commissioning_date) |>
    ggplot(aes(turbine_capacity, value)) +
    geom_hex(bins = 15, alpha = 0.8) +
    geom_smooth(method = "lm") +
    facet_wrap(~name, scales = "free_y") +
    labs(y = NULL) +
    scale_fill_gradient(high = "cyan3") + sin_lineas
#+end_src
#+caption: Gráficos de densidad entre la variable objetivo (eje horizontal) y atributo marcado en el panel. 
#+RESULTS:
[[file:../images/arboles-turbinas-exploratorio.jpeg]]


#+REVEAL: split
Dividimos el conjunto de datos en $50\%$ entrenamiento y $50\%$ prueba.
#+begin_src R :exports none :results none
  set.seed(123)
  wind_split <- initial_split(turbines_df, strata = turbine_capacity, prop = .5)
  wind_train <- training(wind_split)
  wind_test <- testing(wind_split)

  wind_folds <- vfold_cv(wind_train, strata = turbine_capacity)
#+end_src

#+REVEAL: split
Creamos la especificación del modelo, considerando que tenemos el parámetro $\alpha$ como un parámetro especificado por el usuario.

#+begin_src R :exports code :results none
  tree_spec <- decision_tree(
    cost_complexity = tune(),
  ) |>
    set_engine("rpart") |>
    set_mode("regression")

  tree_spec
#+end_src

#+REVEAL: split
Definimos la rejilla donde queremos explorar $\alpha$:
#+begin_src R :exports code :results none 
  tree_grid <- grid_regular(cost_complexity(), levels = 10)
  tree_grid
#+end_src


#+REVEAL: split
Ajustamos el modelo utilizando validación cruzada y la rejilla
#+begin_src R :exports code :results none 
  doParallel::registerDoParallel()
  set.seed(345)
  tree_rs <- tune_grid(
    tree_spec,
    turbine_capacity ~ .,
    resamples = wind_folds,
    grid = tree_grid,
    metrics = metric_set(rmse)
  )
  tree_rs
#+end_src

#+REVEAL: split
Usando validación cruzada podemos cuantificar el error de generalización para cada valor de $\alpha$. Recuerda que de acuerdo a lo que discutimos antes $\alpha$ tiene una incidencia directa en el tamaño del árbol. 
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/turbinas-arboles-validacion-cruzada.jpeg :exports results :results output graphics file
  autoplot(tree_rs) + sin_lineas
#+end_src
#+caption: Error de validación evaluado por validación cruzada para distintos valores de $\alpha$. 
#+RESULTS:
[[file:../images/turbinas-arboles-validacion-cruzada.jpeg]]

#+REVEAL: split
Podemos escoger el mejor modelo de acuerdo a la métrica que definamos:
#+begin_src R :exports code :results none 
  final_tree <- finalize_model(tree_spec, select_best(tree_rs, "rmse"))
  final_tree
#+end_src

#+REVEAL: split
Podemos ajustar el mejor modelo a los datos de entrenamiento o pedirle que ajuste con la separación inicial.
#+begin_src R :exports code :results none 
  final_fit <- fit(final_tree, turbine_capacity ~ ., wind_train)
  final_rs <- last_fit(final_tree, turbine_capacity ~ ., wind_split)
#+end_src

#+REVEAL: split
Por supuesto, no podemos visualizar la respuesta como un modelo de $\mathbb{R}^p \mapsto \mathbb{R}$. Pero podemos escoger las variables mas informativas para la predicción (mas adelante discutimos esto):
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/turbinas-arbol-prediccion.jpeg :exports results :results output graphics file
  ex_fit <- fit(
    final_tree,
    turbine_capacity ~ rotor_diameter_m + commissioning_date,
    wind_train
  )

  wind_train |>
    ggplot(aes(rotor_diameter_m, commissioning_date)) +
    geom_parttree(data = ex_fit, aes(fill = turbine_capacity), alpha = 0.3) +
    geom_jitter(alpha = 0.7, width = 1, height = 0.5, aes(color = turbine_capacity)) +
    scale_colour_viridis_c(aesthetics = c("color", "fill")) + sin_lineas
#+end_src
#+caption: Superficie de respuesta para un modelo simplificado con la configuración encontrada por validación cruzada. 
#+RESULTS:
[[file:../images/turbinas-arbol-prediccion.jpeg]]

#+REVEAL: split
El modelo ajustado es bastante complejo. Por ejemplo, podemos visualizar el árbol y las decisiones:
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/turbinas-arboles-grafico.jpeg  :exports results :results output graphics file :eval never
  final_fit |>
    extract_fit_engine() |>
    rpart.plot(tweak = 2, gap = 0, shadow.col = "gray", branch.lty = 2)
#+end_src
#+caption: Representación gráfica del árbol de decisión. 
#+RESULTS:
[[file:../images/turbinas-arboles-grafico.jpeg]]


* Árboles de clasificación

- La construcción es muy similar a la construcción en el ámbito de regresión.
- Por supuesto, no podemos utilizar el ~RSS~ como métrica de ajuste.
- Podríamos utilizar el ~error de clasificación~ para generar el árbol.
- Pero, el error de clasificación *no* es lo suficientemente sensible para ajustar un árbol.


** Métricas de ajuste: el índice de Gini y devianza

- El ~índice de Gini~ está definido por
  \begin{align}
  G(m) = \sum_{k = 1}^{K} \hat p_{mk} (1 - \hat p_{mk})\,,
  \end{align}
  donde la suma es a través de todas las clases y $\hat p_{mk}$  es la probabilidad de la $k$ ésima clase en la región $m$.
- Toma valores pequeños si todas las $\hat p_{mk}$ son pequeñas o cercanas a 1.
- Por esta, razón, el indice de Gini también se denomina un ~índice de pureza~. Pues nos indica si en un nodo, tenemos una clase *predominante*.


#+REVEAL: split
- Una métrica alternativa es la  ~entropía cruzada~ o devianza
  \begin{align}
  D(m) = - \sum_{k = 1}^{K} \hat p_{mk} \log \hat p_{mk}\,.
  \end{align}
- La cual incide en decisiones similares al índice de Gini en la práctica. 

#+REVEAL: split
#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/pureza-arboles-clasificacion.jpeg :exports results :results output graphics file

  tibble(prob.pred = seq(0.001, 1, length.out = 1000)) |>
    mutate(gini = 2 * prob.pred * (1 - prob.pred),
           entropy = -prob.pred * log(prob.pred) - (1-prob.pred) * log(1 - prob.pred),
           error.rate = ifelse(prob.pred <= .5, prob.pred, 1 - prob.pred)) |>
   pivot_longer(2:4, names_to = ".metric", values_to = "impurity") |>
   ggplot(aes(prob.pred, impurity, group = .metric)) +
    geom_line(aes(colour = .metric), linewidth = 2) + sin_lineas

#+end_src

#+RESULTS:
[[file:../images/pureza-arboles-clasificacion.jpeg]]

** Detalles sobre selección de variables y puntos de corte

Supongamos que tenemos región en el árbol $R$ y medimos su grado de impureza
\begin{align}
i(R) = \begin{cases}
\sum_{k = 1}^{K} p_k(R) \left( 1 - p_k(R) \right) & \text{ índice de Gini } \\
\sum_{k = 1}^{K} p_k(R) \log \left( 1 / p_k(R) \right) & \text{ entropía cruzada }
\end{cases}\,,
\end{align}
donde $p_k(R)$ es la frecuencia relativa de observaciones de la clase $k$ que cae en la región $R$. 


#+REVEAL: split
Para un corte $\mathcal{S} = \{j , s\}$ consideramos la ~ganancia del corte~ como
\begin{align}
\Delta \mathcal{I} (\mathcal{S}, R) = p(R) i(R) - \left[ p(R_1) i(R_1) + p(R_2) i(R_2)\right]\,,
\end{align}
donde como antes $R_1(\mathcal{S}) = \{X | X_j < s\}$ y $R_2(\mathcal{S}) = \{X | X_j \geq s\}$ y buscamos
\begin{align}
\mathcal{S}^\star = \arg \max_{\mathcal{S}} \Delta \mathcal{I} (\mathcal{S}, R)\,.
\end{align}

** Aplicación: Episodios de /Scooby-Doo/

Tomamos el [[https://juliasilge.com/blog/scooby-doo/][siguiente caso]] de Julia Silge para ejemplificar un problema de clasificación con árboles de decisión. El objetivo es predecir si al final del capítulo el monstruo era un monstruo real o era un disfraz. 

#+begin_src R :exports none :results none
  ## Clasificacion: Scooby doo -------------------------------------------------
  tuesdata <- tidytuesdayR::tt_load('2021-07-13')
  scooby_raw <- tuesdata$scoobydoo
#+end_src

#+begin_src R :exports results :results org 
  scooby_raw |>
    filter(monster_amount > 0) |>
    count(monster_real) 
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 2 × 2
  monster_real     n
  <chr>        <int>
1 FALSE          404
2 TRUE           112
#+end_src

#+REVEAL: split
Utilizaremos el año en que salió el episodio y el /rating/ que tuvo ese episodio para predecir si el monstruo era real al final del episodio o no.
#+begin_src R :exports none :results none 
  set.seed(123)
  scooby_split <- scooby_raw |>
    mutate(
      imdb = parse_number(imdb),
      year_aired = lubridate::year(date_aired)
    ) |>
    filter(monster_amount > 0, !is.na(imdb)) |>
    mutate(
      monster_real = case_when(
        monster_real == "FALSE" ~ "fake",
        TRUE ~ "real"
      ),
      monster_real = factor(monster_real)
    ) |>
    select(year_aired, imdb, monster_real, title) |>
    initial_split(strata = monster_real)
  scooby_train <- training(scooby_split)
  scooby_test <- testing(scooby_split)

  set.seed(234)
  scooby_folds <- vfold_cv(scooby_train, strata = monster_real)
#+end_src

#+REVEAL: split
Especificamos el modelo
#+begin_src R :exports code :results none 
  tree_spec <-
    decision_tree(
      cost_complexity = tune(),
      tree_depth = tune(),
      min_n = tune()
    ) |>
    set_mode("classification") |>
    set_engine("rpart")
#+end_src

#+REVEAL: split
Especificamos la rejilla de búsqueda
#+begin_src R :exports both :results org
  tree_grid <- grid_regular(cost_complexity(),
                            tree_depth(),
                            min_n(), levels = 4)
  tree_grid |> print(n = 5)
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 64 × 3
  cost_complexity tree_depth min_n
            <dbl>      <int> <int>
1    0.0000000001          1     2
2    0.0000001             1     2
3    0.0001                1     2
4    0.1                   1     2
5    0.0000000001          5     2
# … with 59 more rows
# ℹ Use `print(n = ...)` to see more rows
#+end_src

#+REVEAL: split
Realizamos el ajuste en cada bloque con cada especificación del modelo.
#+begin_src R :exports code :results none :eval never
  set.seed(345)
  tree_rs <-
    tune_grid(
      tree_spec,
      monster_real ~ year_aired + imdb,
      resamples = scooby_folds,
      grid = tree_grid,
      metrics = metric_set(accuracy, recall, precision, roc_auc)
    )
#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 700 :R-dev-args bg="transparent"
#+begin_src R :file images/scooby-clasificacion-metrics.jpeg :exports results :results output graphics file :eval never
  autoplot(tree_rs) + sin_lineas
#+end_src
#+caption: Resultados de validación cruzada para la configuración de tres parámetros en el modelo de árbol: profundidad, mínimo de observaciones en nodos y penalización por complejidad. 
#+RESULTS:
[[file:../images/scooby-clasificacion-metrics.jpeg]]

#+REVEAL: split
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/scooby-clasificacion-respuesta.jpeg :exports results :results output graphics file :eval never
  simpler_tree <- select_by_one_std_err(tree_rs,
                                        -cost_complexity,
                                        metric = "roc_auc"
                                        )
  final_tree <- finalize_model(tree_spec, simpler_tree)
  final_fit <- fit(final_tree, monster_real ~ year_aired + imdb, scooby_train)

  scooby_train |>
    ggplot(aes(imdb, year_aired)) +
    geom_parttree(data = final_fit, aes(fill = monster_real), alpha = 0.2) +
    geom_jitter(alpha = 0.7, width = 0.05, height = 0.2, aes(color = monster_real))  + sin_lineas
#+end_src
#+caption: Superficie de respuesta por el árbol de decisión. 
#+RESULTS:
[[file:../images/scooby-clasificacion-respuesta.jpeg]]

#+REVEAL: split
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/scooby-clasificacion-arbol.jpeg :exports results :results output graphics file :eval never
  final_fit |>
    extract_fit_engine() |>
    rpart.plot(tweak = 1.5, gap = 0, shadow.col = "gray", branch.lty = 2, extra = 108)
#+end_src
#+caption: Representación gráfica del árbol de decisión. 
#+RESULTS:
[[file:../images/scooby-clasificacion-arbol.jpeg]]

* Hiper-parámetros

Los valores que se determinan por el analista para la construcción de un árbol son:
1. La profundidad máxima o máximo número de cortes.
2. El número de observaciones mas pequeño que puede haber en cada nodo terminal.
3. El costo asociado a complejidad $C_p = \alpha / R(T_0)$.


* Conclusiones

- Los árboles  de decisión son fáciles de interpretar y explicar (siempre y cuando sean pequeños). 
- Algunos piensan que los árboles de decisión refleja el patrón de toma de decisiones de las personas.
- Son fáciles de visualizar, incluso si hay muchos predictores (pueden servir para formar cúmulos).
- Son difíciles de ajustar cuando las relaciones son lineales.
- Las predicciones numéricas pueden ser poco precisas.
- Son sensibles al conjunto de datos que se utilizó para entrenar.
- Hay un componente de selección de variables.
- Son robustos a datos faltantes.
- Se adecúan muy bien a atributos categóricos (siempre y cuando no haya muchas etiquetas).

  
bibliographystyle:abbrvnat
bibliography:references.bib
 
