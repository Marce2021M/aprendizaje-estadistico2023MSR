#+TITLE: EST-25134: Aprendizaje Estadístico
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Modelos aditivos~
#+STARTUP: showall
:REVEAL_PROPERTIES:
# Template uses org export with export option <R B>
# Alternatives: use with citeproc
#+LANGUAGE: es
#+OPTIONS: num:nil toc:nil timestamp:nil
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_MATHJAX_URL: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
#+REVEAL_THEME: night
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Aprendizaje">
#+REVEAL_INIT_OPTIONS: width:1600, height:900, margin:.2
#+REVEAL_EXTRA_CSS: ./mods.css
#+REVEAL_PLUGINS: (notes)
:END:
#+PROPERTY: header-args:R :session nolineal :exports both :results output org :tangle ../rscripts/06-metodos-nolineales.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc latex

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2023 | Modelos aditivos. \\
*Objetivo*: En esta sección estudiaremos modelos que empiezan desviarse de los supuesto lineales. Retendremos el componente aditivo por cuestiones de interpretabilidad. Estudiaremos métodos lineales por regiones, funciones polinomiales y la familia de modelos basados en /splines/ con fines predictivos y como mecanismos de pre-procesamiento. \\
*Lectura recomendada*: Capítulo 7 de citep:James2021. Finalmente, el libro citep:Wood2017 presenta un tratamiento completo sobre modelos aditivos generalizados.
#+END_NOTES

#+begin_src R :exports none :results none
  ## Setup ---------------------------------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)

  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 4)
  ## Problemas con mi consola en Emacs
  options(pillar.subtle = FALSE)
  options(rlang_backtrace_on_error = "none")
  options(crayon.enabled = FALSE)

  ## Para el tema de ggplot
  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
#+end_src

* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
- [[#regresión-polinomial][Regresión polinomial]]
  - [[#ejemplo][Ejemplo:]]
  - [[#incertidumbre-en-predicciones-regresión][Incertidumbre en predicciones (regresión)]]
    - [[#detalles][Detalles]]
  - [[#incertidumbre-en-predicciones-clasificación][Incertidumbre en predicciones (clasificación)]]
  - [[#observaciones][Observaciones]]
- [[#funciones-simples][Funciones simples]]
  - [[#observaciones][Observaciones]]
  - [[#extensiones][Extensiones]]
- [[#modelos-por-segmentos][Modelos por segmentos]]
  - [[#splines][Splines]]
  - [[#splines-lineales][Splines lineales]]
  - [[#splines-cúbicos][Splines cúbicos]]
  - [[#splines-naturales][Splines naturales]]
    - [[#para-pensar][Para pensar]]
  - [[#selección-de-nodos][Selección de nodos]]
- [[#suavizamiento-por-splines][Suavizamiento por splines]]
  - [[#solución][Solución]]
    - [[#bonus][Bonus:]]
  - [[#ajuste-de-suavizador][Ajuste de suavizador]]
- [[#regresión-local][Regresión local]]
  - [[#observaciones][Observaciones]]
- [[#modelo-aditivos-generalizados][Modelo aditivos generalizados]]
  - [[#clasificación][Clasificación]]
- [[#conclusiones][Conclusiones]]
:END:

* Introducción 

Hasta ahora hemos utilizado modelos predictivos basados en ~supuestos
lineales~. Aunque el modelo lineal es un modelo sencillo y bastante útil en la
práctica tiene sus ~limitaciones~. Hemos explorado la idea de hace mas complejo el
modelo lineal --utilizando mas covariables-- utilizando el concepto de
~regularización~ para mantener cierta garantía en la generalización del modelo.  

#+BEGIN_NOTES
Un modelo lineal es una aproximación que muchas veces usamos por
conveniencia. El modelo es conveniente pues representa una aproximación
interpretable a primer orden. Es decir, corresponde a una aproximación lineal de
primer orden de Taylor.
#+END_NOTES

#+REVEAL: split
En esta sección del curso estudiaremos modelos que rompen el supuesto de
linealidad por medio de ~regresión polinomial, funciones simples, splines,
regresión local, y modelos aditivos~.

* Regresión polinomial

Empezamos con un modelo de regresión
\begin{align}
y_i = \beta_0 + \beta_1 x_i + \epsilon_i\,,
\end{align}
el cual /extendemos/ a través de una expresión
\begin{align}
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \cdots + \beta_d x_i^d + \epsilon_i\,.
\end{align}

#+REVEAL: split
Creamos una colección nueva de ~atributos~ $x_i \mapsto x_i, x_i^2, x_i^3, \ldots
x_i^d$ y ajustamos los coeficientes utilizando herramientas de regresión lineal
múltiple.

** Ejemplo:

Datos demográficos y de ingreso para individuos que viven en región central del Atlántico en Estados Unidos. 

#+begin_src R :exports results  :results org 
  ## Regresión polinomial ------------------------------------------------------
  library(ISLR)
  set.seed(108727)
  ## Cargamos datos
  data <- tibble(Wage) |> select(year, age, wage, education) |>
    mutate(hi.income = ifelse(wage > 250, 1, 0),
           age = as.numeric(age))
  data |> 
    print(n = 5) 
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 3,000 × 5
   year   age  wage education       hi.income
  <int> <dbl> <dbl> <fct>               <dbl>
1  2006    18  75.0 1. < HS Grad            0
2  2004    24  70.5 4. College Grad         0
3  2003    45 131.  3. Some College         0
4  2003    43 155.  4. College Grad         0
5  2005    50  75.0 2. HS Grad              0
# … with 2,995 more rows
# ℹ Use `print(n = ...)` to see more rows
#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/wage-polynomial.jpeg :exports results :results output graphics file
  g.reg <- data |>
    ggplot(aes(age, wage)) +
    geom_point(color = "gray") +
    geom_smooth(formula = y ~ poly(x, 4),
                method = "lm", se = TRUE,
                fill = "salmon") + sin_lineas +
    xlab("Edad") + ylab("Ingreso")

  g.log <- data |>
    mutate(wage.plt = ifelse(hi.income == 1, .20, 0 )) |>
    ggplot(aes(age, wage.plt)) +
    geom_point(color = "gray") +
    geom_smooth(aes(age, hi.income),
                formula = y ~ poly(x, 4),
                method = "glm",
                method.args = list(family = "binomial"),
                se = 2, fill = "salmon") + sin_lineas +
    xlab("Edad") + ylab(expression(paste(P,"( Ingreso >",250,"|Edad)", sep = ""))) +
    coord_cartesian(ylim = c(0, 0.20))

  g.reg + g.log
#+end_src
#+name: fig:wage-preds
#+caption: Predicción con polinomio de grado 4 con bandas predictivas. 
#+RESULTS:
[[file:../images/wage-polynomial.jpeg]]

** Incertidumbre en predicciones (regresión)
Nuestro interés es en la capacidad predictiva del modelo (no en los
coeficientes). Nos interesa evaluar
\begin{align}
\hat f(x_0) = \hat \beta_0 + \hat \beta_1 x_0 + \hat \beta_2 x_0^2 + \hat \beta_3 x_0^3 + \hat \beta_4 x_0^4\,,
\end{align}
donde notamos la respuesta es una combinación lineal de los coeficientes. Esto
nos ayuda a calcular
\begin{align}
\mathbb{V}(\hat f(x_o))\,.
\end{align}

#+REVEAL: split
En el panel de regresión ([[fig:wage-preds]]) calculamos intervalos de la forma
\begin{align}
\hat f(x_0) \pm 2 \cdot \mathsf{SE}(\hat f(x_0))\,,
\end{align}
donde $\mathsf{SE}(\hat \theta)$ denota el error estándar del estimador $\hat \theta$. 

*** Detalles
:PROPERTIES:
:reveal_background: #00468b
:END:
Para calcular la varianza de la predicción utilizamos la combinación lineal de los predictores
\begin{align}
\mathbb{V}(\hat f(x_0)) = \mathbb{V}(x_0^\top \hat \beta) = x_0^\top \mathbb{V}(\hat \beta) x_0\,.
\end{align}
** Incertidumbre en predicciones (clasificación)
Para el modelo logístico ([[fig:wage-preds]]) lo que tenemos es
\begin{align}
\mathbb{P}(\mathsf{Ingreso} > 250 | x_i) = \frac{\exp(\beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \cdots + \beta_d x_i^d)}{1 + \exp( \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \cdots + \beta_d x_i^d)}\,.
\end{align}

Los intervalos de confianza se calculan en dos pasos: 1) calculando las cotas en
~escala logit~ y 2) convirtiendo las cotas en ~escala probabilística~.

** Observaciones

- Regresión polinomial requiere una elección para $d$, la cual se puede establecer por medio de validación cruzada.
- Los polinomios, aunque son mas flexibles y ajustan relaciones no lineales, son muy malos para extrapolar.
- Los modelos se ajustan con expresiones en el componente de ~fórmula~ 

  #+begin_src R :exports code :results org :eval never
    y ~ poly(x, degree = 4)
  #+end_src

* Funciones simples

Otra forma de crear transformaciones de variables es considerar distintas regiones y ajustar las medias. Por ejemplo,
considerar ~grupos de edad~: $[18, 35), [35, 50), [50, 65), [65, 100)$ .

#+REVEAL: split
Para lograr esto, generamos una nueva colección de predictores por medio de variables ~categóricas~ o /dummy/
\begin{align}
C_1(X) = I(X < 35), \quad C_2(X) = I(35 \leq X < 50), \quad \ldots, \quad C_4(X \geq 65)\,,
\end{align}
con los que ajustaremos el modelo de regresión (clasificación) lineal
\begin{align}
f(x) = \beta_0 + \beta_1 C_1(x) + \cdots + \beta_4 C_4(x)\,.
\end{align}

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/wage-local-regression.jpeg :exports results :results output graphics file
  ## Regesion constante por regiones ----------------------------
  g.reg <- data |>
    mutate(age.group = cut(age, breaks = c(-Inf, 35, 50, 65, Inf), right = FALSE)) |>
    ggplot(aes(age, wage, group = age.group)) +
    geom_point(color = "gray") + 
    geom_smooth(method = "lm",
                formula = y ~ 1,
                se = TRUE,
                fill = "salmon") + sin_lineas +
    xlab("Edad") + ylab("Ingreso")

  g.log <- data |>
    mutate(wage.plt = ifelse(hi.income == 1, .20, 0 )) |>
    mutate(age.group = cut(age, breaks = c(-Inf, 35, 50, 65, Inf), right = FALSE)) |>
    ggplot(aes(age, wage.plt, group = age.group)) +
    geom_point(color = "gray") + # geom_jitter(color = "gray", width = 1, height = .01) + 
    geom_smooth(aes(age, hi.income),
                formula = y ~ 1,
                method = "glm",
                method.args = list(family = "binomial"),
                se = TRUE,
                fill = "salmon") + sin_lineas +
                xlab("Edad") + ylab(expression(paste(P,"( Ingreso >",250,"|Edad)", sep = "")))

  g.reg + g.log
#+end_src
#+caption: Predicción con regresión local. 
#+RESULTS:
[[file:../images/wage-local-regression.jpeg]]

** Observaciones

- Los parámetros se ajustan de manera /local/. Contrario con los polinomios que ajustan parámetros para todo el rango de los datos. 
- Los modelos se ajustan en cada grupo de edad, donde ~age.group~ es una variable categórica que tiene las indicadoras de los grupos. 

#+begin_src R :exports code :results none :eval never :tangle no
  y ~ age.group
#+end_src

*Nota*: hay que  tener cuidado al querer interpretar, pues en automático se crea un grupo ~base~ pues no queremos tener problemas de multicolinealidad.

#+REVEAL: split
Para graficar (~ggplot2~) basta con pedir la predicción constante con los gráficos agrupados por grupo de edad. Esto se logra con

#+begin_src R :exports code :results none :eval never :tangle no
  ggplot(data, aes(age, wage, group = age.group)) +
  geom_sooth(formula = y ~ 1)
#+end_src

** Extensiones

Una noción natural de incrementar la complejidad del modelo y al mismo tiempo mejorar la capacidad predictiva de éste sería ajustar una recta en cada región, ver [[fig:local-linear]]. 

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/wage-local-linear-regression.jpeg :exports results :results output graphics file
  g.reg <- data |>
    mutate(age.group = cut(age, breaks = c(-Inf, 35, 50, 65, Inf), right = FALSE)) |>
    ggplot(aes(age, wage, group = age.group)) +
    geom_point(color = "gray") + 
    geom_smooth(method = "lm",
                formula = y ~ x,
                se = TRUE,
                fill = "salmon") + sin_lineas +
    xlab("Edad") + ylab("Ingreso")

  g.log <- data |>
    mutate(wage.plt = ifelse(hi.income == 1, .20, 0 )) |>
    mutate(age.group = cut(age, breaks = c(-Inf, 35, 50, 65, Inf), right = FALSE)) |>
    ggplot(aes(age, wage.plt, group = age.group)) +
    geom_point(color = "gray") + # geom_jitter(color = "gray", width = 1, height = .01) + 
    geom_smooth(aes(age, hi.income),
                formula = y ~ x,
                method = "glm",
                method.args = list(family = "binomial"),
                se = TRUE,
                fill = "salmon") + sin_lineas +
    xlab("Edad") + ylab(expression(paste(P,"( Ingreso >",250,"|Edad)", sep = ""))) +
    coord_cartesian(ylim = c(0, 0.20))

  g.reg + g.log
#+end_src
#+caption: Predicción con regresión lineal local.
#+name: fig:local-linear
#+RESULTS:
[[file:../images/wage-local-linear-regression.jpeg]]



* Modelos por segmentos

Uno de los problemas del modelo anterior es que definimos la regresión con
~modelos discontinuos~. Por ejemplo, si ajustamos un modelo donde las regiones
utilicen distintos polinomios. No tendremos garantía de que el modelo completo
se vea conectado. Además, si lo hacemos sin cuidado entonces tendremos modelos
volátiles en las cotas de las regiones de ajuste, ver [[fig:piece-cubic]].

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/wage-local-poly-regression.jpeg :exports results :results output graphics file
  ## Modelos por segmentos -----------------------------
  library(splines)
  g.reg <- data |>
    mutate(age.group = cut(age, breaks = c(-Inf, 50, Inf), right = FALSE)) |>
    ggplot(aes(age, wage, group = age.group)) +
    geom_point(color = "gray") + 
    geom_smooth(method = "lm",
                formula = y ~ poly(x, 2),
                se = TRUE,
                fill = "salmon") + sin_lineas +
    xlab("Edad") + ylab("Ingreso")

  g.log <- data |>
    mutate(wage.plt = ifelse(hi.income == 1, .20, 0 )) |>
    mutate(age.group = cut(age, breaks = c(-Inf, 50, Inf), right = FALSE)) |>
    ggplot(aes(age, wage.plt, group = age.group)) +
    geom_point(color = "gray") + # geom_jitter(color = "gray", width = 1, height = .01) + 
    geom_smooth(aes(age, hi.income),
                formula = y ~ poly(x, 2),
                method = "glm",
                method.args = list(family = "binomial"),
                se = TRUE,
                fill = "salmon") + sin_lineas +
    xlab("Edad") + ylab(expression(paste(P,"( Ingreso >",250,"|Edad)", sep = ""))) +
    coord_cartesian(ylim = c(0, 0.20))


  g.reg + g.log
#+end_src
#+name: fig:piece-cubic
#+caption: Predicción con regresión polinomial de grado 2 en localidades. 
#+RESULTS:
[[file:../images/wage-local-poly-regression.jpeg]]

** /Splines/

Un modelo basado en /splines/ es un modelo basado en polinomios donde se les añade
la propiedad de ~continuidad~ (en las primeras dos derivadas), ver
[[fig:splines-wage]].

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/wage-linear-spline-regression.jpeg :exports results :results output graphics file
  ### Splines ------------------------------------
  g.reg <- data |>
    mutate(age.group = cut(age, breaks = c(-Inf, 50, Inf), right = FALSE)) |>
    ggplot(aes(age, wage)) +
    geom_point(color = "gray") + 
    geom_smooth(method = "lm",
                formula = y ~ bs(x, knots = c(50), degree = 2),
                se = TRUE,
                fill = "salmon") + sin_lineas +
    geom_vline(xintercept = 50, lty = 2) + 
    xlab("Edad") + ylab("Ingreso")

  g.log <- data |>
    mutate(wage.plt = ifelse(hi.income == 1, .20, 0 )) |>
    mutate(age.group = cut(age, breaks = c(-Inf, 50, Inf), right = FALSE)) |>
    ggplot(aes(age, wage.plt)) +
    geom_point(color = "gray") + # geom_jitter(color = "gray", width = 1, height = .01) + 
    geom_smooth(aes(age, hi.income),
                formula = y ~ bs(x, knots = c(50), degree = 2),
                method = "glm",
                method.args = list(family = "binomial"),
                se = TRUE,
                fill = "salmon") + sin_lineas +
    geom_vline(xintercept = 50, lty = 2) + 
    xlab("Edad") + ylab(expression(paste(P,"( Ingreso >",250,"|Edad)", sep = ""))) +
    coord_cartesian(ylim = c(0, 0.20))


  g.reg + g.log
#+end_src
#+name: fig:splines-wage
#+caption: Predicción con regresión /splines/ de grado 2. La línea punteada marca el punto donde se /conectan/ los dos polinomios.
#+RESULTS:
[[file:../images/wage-linear-spline-regression.jpeg]]

** /Splines/ lineales

Los /splines/ de grado 1 son funciones lineales continuas por segmentos. Se construyen a través de ~funciones base~
\begin{align}
b_1(x) &= x \\
b_{k+1 }(x) &= (x - \xi_k)_+, \qquad k = 1, \ldots, K\,,
\end{align}
y una colección de ~nodos~ $\xi_k$, donde $(\cdot)_+$ denota la ~parte positiva~ de la función.

De tal manera que el modelo predictivo queda en términos de
\begin{align}
y_i = \beta_0 + \beta_1 b_1(x_i) + \cdots + \beta_{K+1} b_{K+1}(x_i) + \epsilon_i\,.
\end{align}

** /Splines/ cúbicos 

Los /splines/ de grado 3 son funciones  continuas por segmentos. Se construyen a través de ~funciones base~
\begin{gather}
b_1(x) = x \,,\\
b_2(x) = x^2\,,\\
b_3(x) = x^3\,,\\
b_{k+3 }(x) = (x - \xi_k)_+^3, \qquad k = 1, \ldots, K\,,
\end{gather}
y una colección de ~nodos~ $\xi_k$, donde $(\cdot)_+^3$ denota la ~parte positiva~ de la función.

*Nota* que en cada nodo la función construida tiene a lo más 2 derivadas continuas. 

#+REVEAL: split
De tal manera que el modelo predictivo queda en términos de
\begin{align}
y_i = \beta_0 + \beta_1 b_1(x_i) + \cdots + \beta_{K+3} b_{K+3}(x_i) + \epsilon_i\,.
\end{align}

** /Splines/ naturales

Un /spline/ ~natural~ es un /spline/ con la restricción adicional de considerar una
~extrapolación lineal~ fuera de los ~nodos frontera~. Ver [[fig:nsplines-wage]].

#+REVEAL: split
#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/wage-natural-spline-regression.jpeg :exports results :results output graphics file
  ### Splines naturales --------------------------
  set.seed(108727)
  g.cubic <- data |>
    sample_frac(.05) |>
    ggplot(aes(age, wage)) +
    geom_point(color = "gray") + 
    stat_smooth(aes(age, wage, fill = "Spline"), color = 'salmon',
                method = "lm",
                formula = y ~ bs(x, knots = c(35, 50, 65), degree = 3),
                se = TRUE, lty = 1, alpha = .2, fullrange = TRUE) +
    stat_smooth(aes(age, wage, fill = "Spline-natural"),
                method = "lm",
                formula = y ~ ns(x, knots = c(35, 50, 65)), color = 'blue',
                se = TRUE, lty = 1, alpha = .2, fullrange = TRUE) + sin_lineas +
    geom_vline(xintercept = c(35, 50, 65), lty = 2) +
    scale_x_continuous(limits = c(10, 80), expand = c(0,0)) +
    xlab("Edad") + ylab("Ingreso") +
  coord_cartesian(ylim = c(0, 300))

  g.cubic
#+end_src
#+name: fig:nsplines-wage
#+caption: Predicción con regresión utilizando /splines/ de grado 3. Las líneas punteadas representan los nodos ($\xi_k$) del modelo. 
#+RESULTS:
[[file:../images/wage-natural-spline-regression.jpeg]]

*** Para pensar
:PROPERTIES:
:reveal_background: #00468b
:END:
Para el caso de regresión $f: \mathbb{R} \rightarrow \mathbb{R}$, incorporar un /spline/ natural agrega $4 = 2\times 2$ restricciones adicionales, ¿por qué?

** Selección de nodos
- Una estrategia es elegir el número de nodos $K$ y después utilizar los percentiles correspondientes de $X$.
- Un /spline/ cúbico con $K$ nodos tiene $K+4$ parámetros (o grados de libertad).
- Un /spline/ natural con $K$ nodos tiene $K$ parámetros (o grados de libertad).


#+REVEAL: split
Utilizando la noción de grados de libertad podemos comparar un polinomio con un
grado predeterminado y un spline natural con un número de nodos fijo.
#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/wages-splines-polinomio.jpeg :exports results :results output graphics file
    g.polsplines <- data |>
      ggplot(aes(age, wage)) +
      geom_point(color = "gray") + 
      stat_smooth(aes(age, wage, fill = "Polinomio"),
                  color = 'salmon',
                  method = "lm",
                  formula = y ~ poly(x, 14),
                  se = TRUE, lty = 1,
                  alpha = .2, fullrange = TRUE) +
      stat_smooth(aes(age, wage, fill = "Spline-natural"),
                  method = "lm",
                  formula = y ~ ns(x, df = 14),
                  color = 'blue',
                  se = TRUE, lty = 1,
                  alpha = .2, fullrange = TRUE) + sin_lineas +
      scale_x_continuous(limits = c(10, 80), expand = c(0,0)) +
      xlab("Edad") + ylab("Ingreso") +
      coord_cartesian(ylim = c(0, 300))

  g.polsplines
#+end_src
#+caption: Ajuste con modelos con 15 grados de libertad. Polinomio de potencia 14, y /spline/ natural (cúbico).
#+RESULTS:
[[file:../images/wages-splines-polinomio.jpeg]]

* Suavizamiento por /splines/

Consideremos el problema de ~ajustar un función continua y diferenciable~ $g(\cdot)$ a un ~conjunto de datos~. Lo cual logramos por medio de
\begin{align}
\min_{g \in \mathcal{S}} \sum_{i = 1}^{n} (y_i - g(x_i))^2 + \lambda \int g''(t)^2 dt\,.
\end{align}

- ¿Qué rol juega $\lambda$?

** Solución 

- La ~solución~ es un /spline/ natural con polinomios cúbicos. Los nodos se localizan en cada uno de los datos de entrenamiento $x_i$. La suavidad del estimador es controlada por medio de $\lambda$.

 #+BEGIN_NOTES
   El término de regularización afecta directamente en los coeficientes. Logrando así, eliminar la aparente complejidad de considerar tantos nodos como observaciones tengamos. 
 #+END_NOTES

- El vector de $n$ predicciones se puede escribir como
  \begin{align}
  \hat g_\lambda = S_\lambda y\,.
  \end{align}

 #+BEGIN_NOTES
   La solución al problema de optimización se logra por medio de $f(x) = \sum_{j = 1}^{N} \beta_j \, N_j(x)$ donde $N_j(\cdot)$ denota la base de funciones para el espacio de /splines/ naturales. De esta manera la función objetivo se puede reescribir como
    \begin{align}
    \mathcal{J}(\beta) = (y - N\beta)^\top (y - N\beta) + \lambda \beta^\top \Omega_N \beta\,,
   \end{align}
  donde $\{\Omega_N\}_{jk} = \int N''_j(t) N''_k(t) \text{d}t$, cuya solución se puede escribir de manera analítica.  
 #+END_NOTES

  
- El ~número efectivo de grados de libertad~  se puede calcular a través de
  \begin{align}
   \mathsf{df}_\lambda = \sum_{i = 1}^{n} \{S_\lambda\}_{ii}\,.
  \end{align}
 #+BEGIN_NOTES
  Los grados de libertad efectivos en el contexto de /splines/  fueron definidos (citep:Hastie2009c) en analogía con que $M = \text{tr}(H) = \text{tr}(X(X^\top X)^{-1} X^\top)$ nos da la dimensión del espacio en donde se proyectan las predicciones de mínimos cuadrados para el modelo lineal. 
 #+END_NOTES

*** /Bonus/:
:PROPERTIES:
:reveal_background: #00468b
:END:

El error de validación cruzada se puede calcular por medio de
\begin{align}
\mathsf{RSS}_{\mathsf{CV }}(\lambda) = \sum_{i =1}^{n} \left(y_i - \hat g_{\lambda}^{(-i)}(x_i)\right)^2 = \sum_{i = 1}^{n} \left[ \frac{y_i - \hat g_\lambda(x_i)}{1 - \{S_\lambda\}_{ii}}\right]^2\,.
\end{align}

** Ajuste de suavizador 

Para ajustar el suavizador podemos ~controlar por los grados de libertad~
($\mathsf{df}_\lambda$) en lugar de utilizar el coeficiente de penalización de
curvatura. Esto es por que existe una ~relación inversa~ entre $\lambda$ y $\mathsf{df}_\lambda$ . 

#+BEGIN_NOTES
Establecer la relación entre los grados de libertad y el coeficiente de
penalización escapa los intereses del curso. Se puede encontrar en el Capitulo 5
de citep:Hastie2009c. La idea general la esbozamos a continuación. Utilizamos
$\mathsf{df}_\lambda = \mathsf{tr}(S_\lambda)$ donde además sabemos que la traza
de una matriz es la suma de sus eigenvalores. Utilizaremos analogía con
regresión y regularización $L_2$. La solución de mínimos cuadrados bajo
regresión Ridge es $\hat \beta = (X^\top X + \lambda I)^{-1}X^\top y$. Las
predicciones las realizamos por medio de $X\hat \beta$, de donde podemos
escribir el vector de predicciones $\tilde y = H_\lambda y$. Utilizando la
descomposición espectral de $X \in \mathbb{R}^{n\times p}$ podemos encontrar los eigenvalores de $H_\lambda$ y en consecuencia veremos una relación 
entre los grados de libertad ($\mathsf{df}_\lambda$) y el coeficiente de penalización ($\lambda$). 
#+END_NOTES


#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/wages-smoothing-splines.jpeg :exports results :results output graphics file
  ## Suavizamiento (splines) --------------------
  library(ggformula)
  set.seed(108727)
  g1.ssplines <- data |>
    sample_frac(.05)|>
    ggplot(aes(age, wage)) +
    geom_point(color = "gray") +
    geom_spline(aes(age, wage, color = "Suavizamiento"),
              df = 2, 
              color = 'red',
              lty = 1,
              show.legend = TRUE) + 
    sin_lineas +
    ## scale_x_continuous(limits = c(10, 80), expand = c(0,0)) +
    xlab("Edad") + ylab("Ingreso") + ggtitle("df = 2")
    coord_cartesian(ylim = c(0, 300))

  set.seed(108727)
  g2.ssplines <- data |>
    sample_frac(.05)|>  
    ggplot(aes(age, wage)) +
    geom_point(color = "gray") +
    geom_spline(aes(age, wage, color = "Suavizamiento"),
              df = 15, 
              color = 'red',
              se = TRUE, lty = 1,
              fullrange = TRUE, show.legend = TRUE) + 
    sin_lineas +
    ## scale_x_continuous(limits = c(10, 80), expand = c(0,0)) +
    xlab("Edad") + ylab("Ingreso") + ggtitle("df = 15")
    coord_cartesian(ylim = c(0, 300)) 

  set.seed(108727)
  g3.ssplines <- data |>
    sample_frac(.05)|>  
    ggplot(aes(age, wage)) +
    geom_point(color = "gray") +
    geom_spline(aes(age, wage, color = "Suavizamiento"),
              df = 40, 
              color = 'red',
              se = TRUE, lty = 1,
              fullrange = TRUE, show.legend = TRUE) + 
    sin_lineas +
    ## scale_x_continuous(limits = c(10, 80), expand = c(0,0)) +
    xlab("Edad") + ylab("Ingreso") + ggtitle("df = 40")
    coord_cartesian(ylim = c(0, 300)) 

  g1.ssplines + g2.ssplines + g3.ssplines
#+end_src
#+caption: Suavizamiento por /splines/. Controlamos por grados de libertad ($\mathsf{df}_\lambda$). 
#+RESULTS:
[[file:../images/wages-smoothing-splines.jpeg]]

* Regresión local

El objetivo es: ajustar un ~modelo por regiones~ donde tengamos una función de peso que sólo considere una vecindad. El ajuste se realiza por medio de mínimos cuadrados ~ponderados~. Los pesos alrededor de un punto base  $x_0$ usualmente están definidos por *funciones /kernel/* $K_\lambda(x, x_0)$
donde $\lambda$ determina el tamaño de la vecindad.

#+REVEAL: split
El problema de regresión que se resuelve es un problema puntual en cada $x_0$ que queramos evaluar
\begin{align}
 \min_{{\color{orange} \alpha}(x_0), {\color{orange} \beta} (x_0)} \sum_{i = 1}^{N} K_\lambda(x_0, x_i) [y_i - {\color{orange} \alpha} (x_0) - {\color{orange} \beta} (x_0) x_i]^2\,.
\end{align}

#+REVEAL: split
La solución a este problema está dada por mínimos cuadrados ~ponderados~ donde en especifico tenemos la expresión
\begin{align*}
\hat f(x_0) &= b(x_0)^\top (B^\top W(x_0) B)^{-1} B^\top W(x_0) y \,,\\
&= \sum_{i=1}^{N} c_i(x_0) y_i\,.
\end{align*}

#+BEGIN_NOTES
El planteamiento y la solución de este problema de regresión se conoce como el suavizador de Watson-Nadaraya suavizado citep:Hastie2009c. Aunque parezca un problema con una vasta historia aún sigue siendo motivo de estudio. De hecho su aplicación se encuentra en el centro de los modelos del estado del arte en Procesamiento de Lenguaje Natural (NLP, por sus siglas en inglés) como BERT, GPT-3, etc. citep:Zhang2021c.
#+END_NOTES

#+REVEAL: split
De modo que se ajusta un modelo lineal de manera local ([[fig:loess-local]]). La vecindad está controlada por un parámetro ~span~ que dicta el porcentaje de puntos de entrenamiento alrededor de donde queremos hacer predicciones.

#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/wage-regresion-loess.jpeg :exports results :results output graphics file
  ## Suavizamiento (regresion local) ----------------------------------
  set.seed(108727)
  data.plt <- data |>
    sample_frac(.1) |>
    mutate(region = ifelse((20 <= age & age <= 30),
                           TRUE, FALSE))
    g1 <- data.plt |>
      ggplot(aes(age, wage)) +
      geom_smooth(method = "loess",
                  span = .35,
                  method.args = list(degree = 1),
                  color = 'blue', 
                  se = TRUE, lty = 1,
                  alpha = .2, fullrange = TRUE) +
      geom_smooth(data = filter(data.plt, region),
                  aes(age, wage),
                  method = "loess",
                  span = 10,
                  method.args = list(degree = 1),
                  color = 'red', fill = 'red',
                  se = TRUE, lty = 1,
                  alpha = .2, fullrange = TRUE) +
      geom_point(color = "gray", shape = 4) +
      geom_point(data = filter(data.plt, region),
                  aes(age, wage),
                 color = "salmon") + 
      sin_lineas +
      xlab("Edad") + ylab("Ingreso") +
      coord_cartesian(ylim = c(0, 300)) +
      ggtitle("Centro en x = 25")

  set.seed(108727)
  data.plt <- data |>
    sample_frac(.1) |>
    mutate(region = ifelse((35 <= age & age <= 45),
                             TRUE, FALSE))
  g2 <- data.plt |>
    ggplot(aes(age, wage)) +
    geom_smooth(method = "loess",
                span = .35,
                method.args = list(degree = 1),
                  color = 'blue', 
                se = TRUE, lty = 1,
                alpha = .2, fullrange = TRUE) +
    geom_smooth(data = filter(data.plt, region),
                aes(age, wage),
                method = "loess",
                  span = 10,
                method.args = list(degree = 1),
                color = 'red', fill = 'red',
                se = TRUE, lty = 1,
                alpha = .2, fullrange = TRUE) +
    geom_point(color = "gray", shape = 4) +
    geom_point(data = filter(data.plt, region),
               aes(age, wage),
               color = "salmon") + 
    sin_lineas +
    xlab("Edad") + ylab("Ingreso") +
    coord_cartesian(ylim = c(0, 300)) +
      ggtitle("Centro en x = 40")

  g1 + g2
#+end_src
#+name: fig:loess-local
#+caption: Regresión local con ventana móvil.
#+RESULTS:
[[file:../images/wage-regresion-loess.jpeg]]

#+REVEAL: split
Cambiar el radio de las vecindades afecta la complejidad del modelo que usaremos para predecir ([[fig:loess-span]]). 
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/wage-regresion-loess-span.jpeg :exports results :results output graphics file
    set.seed(108727)
    data.plt <- data |>
      sample_frac(.1) |>
      mutate(region = ifelse((20 <= age & age <= 30),
                             TRUE, FALSE))
    g1 <- data.plt |>
      ggplot(aes(age, wage)) +
      geom_smooth(method = "loess",
                  span = .05,
                  method.args = list(degree = 1),
                  color = 'blue', 
                  se = TRUE, lty = 1,
                  alpha = .2, fullrange = TRUE) +
      geom_point(color = "gray", shape = 4) +
      sin_lineas +
      xlab("Edad") + ylab("Ingreso") +
      coord_cartesian(ylim = c(0, 300)) +
      ggtitle("Span = 0.05")

    g2 <- data.plt |>
      ggplot(aes(age, wage)) +
      geom_smooth(method = "loess",
                  span = .15,
                  method.args = list(degree = 1),
                  color = 'blue', 
                  se = TRUE, lty = 1,
                  alpha = .2, fullrange = TRUE) +
      geom_point(color = "gray", shape = 4) +
      sin_lineas +
      xlab("Edad") + ylab("Ingreso") +
      coord_cartesian(ylim = c(0, 300)) +
      ggtitle("Span = 0.15")

    g3 <- data.plt |>
      ggplot(aes(age, wage)) +
      geom_smooth(method = "loess",
                  span = 0.5,
                  method.args = list(degree = 1),
                  color = 'blue', 
                  se = TRUE, lty = 1,
                  alpha = .2, fullrange = TRUE) +
      geom_point(color = "gray", shape = 4) +
      sin_lineas +
      xlab("Edad") + ylab("Ingreso") +
      coord_cartesian(ylim = c(0, 300)) +
      ggtitle("Span = 0.50")

  g1 + g2 + g3
#+end_src
#+name: fig:loess-span
#+caption: Regresión local con amplitud variable.
#+RESULTS:
[[file:../images/wage-regresion-loess-span.jpeg]]


** Observaciones

- En la práctica un suavizador por splines (~smooth.spline~) o un modelo de regresión local (~loess~) tienen un comportamiento similar. 


* Modelo aditivos generalizados

La estructura aditiva se mantiene y nos permite incorporar una estructura predictiva en cada componente 
\begin{align}
y_i = \beta_0 + \beta_1 f_{1}(x_{i1}) + \cdots + \beta_p f_{p}(x_{ip}) + \epsilon_{i} \,.
\end{align}

#+REVEAL: split
#+HEADER: :width 1200 :height 600 :R-dev-args bg="transparent"
#+begin_src R :file images/wage-regresion-gam.jpeg :exports results :results output graphics file
  library(mgcv)
  library(mgcViz)
  set.seed(108727)
  data.plt <- data |>
    sample_frac(.75) |>
    mutate(year = as.numeric(year),
           education = factor(as.numeric(education))) 

  gam.model <- gam(wage ~ ns(year, df = 5) +
                     ns(age, df = 5) +
                     education, data = data.plt)
  b <- getViz(gam.model)
  print(plot(b, allTerms = TRUE) +
        l_fitLine(linetype = 1) +      
        l_ciLine(mul = 1, linetype = 3) + 
        l_ciPoly(mul = 2) +
        l_rug(alpha = 0.8) +
        ## l_points(shape = 19, size = 1, alpha = 0.1) +
        l_ciBar(mul = 2) + l_fitPoints(size = 1, col = 2) +
        theme_get() + sin_lineas,
        pages = 1)
#+end_src
#+caption: Regresión con modelo aditivo con tres componentes. 
#+RESULTS:
[[file:../images/wage-regresion-gam.jpeg]]

** Clasificación

La linealidad se mantiene y se pueden explorar las contribuciones de cada término en escala ~logit~: 

\begin{align}
\log \left( \frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 f_{1}(x_{i1}) + \cdots + \beta_p f_{p}(x_{ip}) \,.
\end{align}

* Conclusiones

Los modelos estudiados en esta sección son modelos que pertenecen a la ~familia
de modelos aditivos~. Son relativamente sencillos de explorar y criticar (al
visualizar o estudiar cada uno de los términos). La sección siguiente del curso estudiaremos modelos
que utilizan una estructura muy particular para los términos $f_i(x)$. 

bibliographystyle:abbrvnat
bibliography:references.bib
